import os
import json
import asyncio
import pandas as pd
import aiohttp
from datetime import datetime
from io import BytesIO
from pathlib import Path
from typing import List, Dict, Any, Optional
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, StarTools, register
from astrbot.api import logger
from astrbot.core.platform.message_type import MessageType
import base64

@register(
    "astrbot_plugin_group_backup",
    "Foolllll",
    "ç¾¤å¤‡ä»½æ’ä»¶ï¼Œå¤‡ä»½ç¾¤æˆå‘˜ã€å…¬å‘Šã€ç²¾åç­‰æ•°æ®",
    "0.0.1",
    "https://github.com/Foolllll-J/astrbot_plugin_group_backup"
)
class GroupBackupPlugin(Star):
    def __init__(self, context: Context, config: Optional[Dict] = None):
        super().__init__(context)
        self.config = config if config else {}
        self.plugin_data_dir = StarTools.get_data_dir("astrbot_plugin_group_backup")
        self.admin_users = [int(u) for u in self.config.get("admin_users", [])]
        self.download_semaphore = asyncio.Semaphore(5) # é™åˆ¶å¹¶å‘ä¸‹è½½æ•°
        self.default_backup_options = ["ç¾¤ä¿¡æ¯", "ç¾¤å¤´åƒ", "ç¾¤æˆå‘˜", "ç¾¤å…¬å‘Š", "ç²¾åæ¶ˆæ¯", "ç¾¤ç›¸å†Œ", "ç¾¤è£èª‰"]
        
        # å­—æ®µæ˜ å°„ï¼šé…ç½®é¡¹å -> API è¿”å›çš„é”®å
        self.field_map = {
            "QQå·": "user_id",
            "æ˜µç§°": "nickname",
            "ç¾¤æ˜µç§°": "card",
            "æƒé™": "role",
            "åŠ ç¾¤æ—¶é—´": "join_time",
            "æœ€åå‘è¨€": "last_sent_time",
        }

    @property
    def backup_options(self) -> List[str]:
        return self.config.get("backup_options", self.default_backup_options)

    def _format_timestamp(self, timestamp):
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        if isinstance(timestamp, (int, float)) and timestamp > 0:
            return datetime.fromtimestamp(float(timestamp)).strftime("%Y-%m-%d %H:%M:%S")
        return "æœªçŸ¥"

    async def _download_file(self, url: str, save_path: Path, overwrite: bool = False):
        """ä¸‹è½½æ–‡ä»¶ï¼Œå¦‚æœå·²å­˜åœ¨ä¸”æœªå¼€å¯ overwrite åˆ™è·³è¿‡"""
        if not overwrite and save_path.exists():
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {save_path}")
            return True
        
        async with self.download_semaphore:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, timeout=300) as response:
                        if response.status == 200:
                            content = await response.read()
                            
                            # å¦‚æœæ˜¯è¦†ç›–æ¨¡å¼ï¼Œä¸”æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
                            if overwrite and save_path.exists():
                                import hashlib
                                with open(save_path, "rb") as f:
                                    old_content = f.read()
                                if hashlib.md5(content).hexdigest() == hashlib.md5(old_content).hexdigest():
                                    return False # å†…å®¹æ— å˜åŒ–
                            
                            save_path.parent.mkdir(parents=True, exist_ok=True)
                            with open(save_path, "wb") as f:
                                f.write(content)
                            logger.info(f"æˆåŠŸä¿å­˜æ–‡ä»¶: {save_path}")
                            return True # å†…å®¹æœ‰å˜åŒ–æˆ–æ–°ä¸‹è½½
                        else:
                            logger.warning(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥ {url}: HTTP {response.status}")
            except Exception as e:
                logger.error(f"ä¸‹è½½è¿‡ç¨‹å‡ºé”™ {url}: {e}")
        return False

    def _get_latest_backup_data(self, group_id: int) -> Dict[str, Any]:
        """è·å–æœ€è¿‘ä¸€æ¬¡å¤‡ä»½çš„æ•°æ®"""
        group_dir = Path(self.plugin_data_dir) / str(group_id)
        if not group_dir.exists():
            return {}
        
        # æŸ¥æ‰¾æ—¶é—´æˆ³ç›®å½•
        backups = [d for d in group_dir.iterdir() if d.is_dir() and d.name.replace("_", "").isdigit()]
        if not backups:
            return {}
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        latest_backup_dir = sorted(backups, key=lambda x: x.name)[-1]
        
        data = {}
        for file_path in latest_backup_dir.glob("*.json"):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = json.load(f)
                    data[file_path.stem] = content
            except Exception as e:
                logger.warning(f"åŠ è½½ä¸Šä¸€æ¬¡å¤‡ä»½æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        
        return data

    def _append_log(self, group_id: int, log_name: str, log_entry: Dict[str, Any]):
        """è¿½åŠ æ—¥å¿—è®°å½•"""
        log_dir = Path(self.plugin_data_dir) / str(group_id) / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = log_dir / f"{log_name}.json"
        
        logs = []
        if log_file.exists():
            try:
                with open(log_file, "r", encoding="utf-8") as f:
                    logs = json.load(f)
            except:
                logs = []
        
        logs.append({
            "log_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            **log_entry
        })
        
        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(logs, f, ensure_ascii=False, indent=4)
        logger.info(f"å·²è¿½åŠ æ—¥å¿—åˆ° {log_file}: {log_entry}")

    def _archive_deleted_items(self, group_id: int, item_type: str, items: List[Any]):
        """å½’æ¡£å·²åˆ é™¤çš„é¡¹ç›®åˆ°å›æ”¶ç«™"""
        archive_dir = Path(self.plugin_data_dir) / str(group_id) / "logs"
        archive_dir.mkdir(parents=True, exist_ok=True)
        archive_file = archive_dir / "deleted_items.json"
        
        archive = {}
        if archive_file.exists():
            try:
                with open(archive_file, "r", encoding="utf-8") as f:
                    archive = json.load(f)
            except:
                archive = {}
        
        if item_type not in archive:
            archive[item_type] = []
            
        for item in items:
            archive[item_type].append({
                "deleted_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "content": item
            })
            
        with open(archive_file, "w", encoding="utf-8") as f:
            json.dump(archive, f, ensure_ascii=False, indent=4)
        logger.info(f"å·²å½’æ¡£ {len(items)} ä¸ªå·²åˆ é™¤çš„é¡¹ç›®ï¼ˆç±»å‹: '{item_type}'ï¼‰åˆ° {archive_file}")

    @filter.command("ç¾¤å¤‡ä»½")
    async def group_backup(self, event: AstrMessageEvent, group_id_arg: str = ""):
        """ç¾¤å¤‡ä»½ [ç¾¤å·]ï¼šå¤‡ä»½å½“å‰ç¾¤æˆ–æŒ‡å®šç¾¤æ•°æ®åˆ°æœ¬åœ° JSON"""
        # æƒé™æ£€æŸ¥ï¼šBot ç®¡ç†å‘˜ æˆ– é…ç½®é¡¹ä¸­çš„ç®¡ç†å‘˜
        is_admin = event.is_admin()
        user_id = int(event.get_sender_id())
        if not is_admin and (not self.admin_users or user_id not in self.admin_users):
            yield event.plain_result(f"âŒ æ­¤æŒ‡ä»¤ä»…é™ç®¡ç†å‘˜ä½¿ç”¨")
            return

        target_group_id = group_id_arg.strip()
        if not target_group_id:
            target_group_id = event.get_group_id()
        
        if not target_group_id:
            yield event.plain_result("è¯·åœ¨ç¾¤èŠä¸­ä½¿ç”¨æ­¤æŒ‡ä»¤ï¼Œæˆ–åœ¨æŒ‡ä»¤åè·Ÿéšç¾¤å·ã€‚")
            return

        try:
            group_id = int(target_group_id)
            client = event.bot
            
            yield event.plain_result(f"æ­£åœ¨å¼€å§‹å¤‡ä»½ç¾¤ {group_id} çš„æ•°æ®...")

            # åŠ è½½ä¸Šä¸€æ¬¡å¤‡ä»½çš„æ•°æ®ç”¨äºå¢é‡å¯¹æ¯”
            latest_data = self._get_latest_backup_data(group_id)
            
            # 1. è·å–è¯¦ç»†ä¿¡æ¯ (åŒ…å«åŸºç¡€ä¿¡æ¯)
            group_detail = {}
            if "ç¾¤ä¿¡æ¯" in self.backup_options:
                try:
                    raw_detail = await client.api.call_action("get_group_detail_info", group_id=group_id)
                    logger.debug(f"API å“åº” (get_group_detail_info): {json.dumps(raw_detail, ensure_ascii=False)}")
                    
                    # ç²¾ç®€ç¾¤è¯¦ç»†ä¿¡æ¯
                    essential_detail_keys = [
                        "groupCode", "groupName", "ownerUin", "memberNum", "maxMemberNum", 
                        "groupMemo", "groupCreateTime", "activeMemberNum", "groupGrade",
                        "group_all_shut", "groupClassText"
                    ]
                    group_detail = {k: raw_detail.get(k) for k in essential_detail_keys if k in raw_detail}
                            
                except Exception as e:
                    logger.warning(f"è·å–ç¾¤ä¿¡æ¯å¤±è´¥: {e}")

            # 1.1 è·å–ç¾¤å¤´åƒ
            if "ç¾¤å¤´åƒ" in self.backup_options:
                try:
                    avatar_url = f"http://p.qlogo.cn/gh/{group_id}/{group_id}/640/"
                    avatar_dir = Path(self.plugin_data_dir) / str(group_id)
                    avatar_save_path = avatar_dir / "group_avatar.png"
                    temp_avatar_path = avatar_dir / "temp_avatar.png"
                    
                    # å…ˆä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
                    await self._download_file(avatar_url, temp_avatar_path, overwrite=True)
                    
                    if temp_avatar_path.exists():
                        import hashlib
                        is_updated = True
                        if avatar_save_path.exists():
                            with open(avatar_save_path, "rb") as f:
                                old_md5 = hashlib.md5(f.read()).hexdigest()
                            with open(temp_avatar_path, "rb") as f:
                                new_md5 = hashlib.md5(f.read()).hexdigest()
                            
                            if old_md5 == new_md5:
                                is_updated = False
                        
                        if is_updated:
                            if avatar_save_path.exists():
                                # å½’æ¡£æ—§å¤´åƒ
                                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                                archive_path = avatar_dir / "logs" / "deleted_items" / f"avatar_{timestamp}.png"
                                archive_path.parent.mkdir(parents=True, exist_ok=True)
                                import shutil
                                shutil.copy2(avatar_save_path, archive_path)
                                self._append_log(group_id, "content_changes", {"type": "ç¾¤å¤´åƒæ›´æ–°", "old_avatar": archive_path.name})
                                logger.info(f"æ£€æµ‹åˆ°ç¾¤å¤´åƒæ›´æ–°ï¼Œæ—§å¤´åƒå·²å½’æ¡£: {archive_path.name}")
                            
                            # åº”ç”¨æ–°å¤´åƒ
                            if avatar_save_path.exists(): avatar_save_path.unlink()
                            temp_avatar_path.rename(avatar_save_path)
                        else:
                            # æ— å˜åŒ–ï¼Œåˆ é™¤ä¸´æ—¶æ–‡ä»¶
                            temp_avatar_path.unlink()
                except Exception as e:
                    logger.warning(f"è·å–ç¾¤å¤´åƒå¤±è´¥: {e}")

            # 2. è·å–æˆå‘˜åˆ—è¡¨å¹¶ç²¾ç®€
            members = []
            if "ç¾¤æˆå‘˜" in self.backup_options:
                raw_members = await client.get_group_member_list(group_id=group_id)
                logger.debug(f"API å“åº” (get_group_member_list): è·å–åˆ° {len(raw_members)} åæˆå‘˜ã€‚")
                essential_keys = ["user_id", "nickname", "card", "role", "join_time", "last_sent_time"]
                for m in raw_members:
                    members.append({k: m.get(k) for k in essential_keys if k in m})
                
                # å¢é‡å¯¹æ¯”ç¾¤æˆå‘˜
                if latest_data and "members" in latest_data:
                    old_members_map = {m["user_id"]: m for m in latest_data["members"]}
                    new_members_map = {m["user_id"]: m for m in members}
                    
                    # è°è¿›ç¾¤äº†
                    joiners = [m for uid, m in new_members_map.items() if uid not in old_members_map]
                    # è°é€€ç¾¤äº†
                    leavers = [m for uid, m in old_members_map.items() if uid not in new_members_map]
                    
                    if joiners:
                        logger.info(f"æ£€æµ‹åˆ°æ–°æˆå‘˜è¿›ç¾¤: {joiners}")
                        for m in joiners:
                            self._append_log(group_id, "member_changes", {"type": "å…¥ç¾¤", "user_id": m["user_id"], "nickname": m.get("nickname")})
                    if leavers:
                        logger.info(f"æ£€æµ‹åˆ°æˆå‘˜é€€ç¾¤: {leavers}")
                        for m in leavers:
                            self._append_log(group_id, "member_changes", {"type": "é€€ç¾¤", "user_id": m["user_id"], "nickname": m.get("nickname")})
            
            # 3. è·å–å…¬å‘Š
            notices = []
            if "ç¾¤å…¬å‘Š" in self.backup_options:
                try:
                    # ä½¿ç”¨ä¸‹åˆ’çº¿å¼€å¤´çš„ API é€šå¸¸éœ€è¦ call_action
                    raw_notices = await client.api.call_action("_get_group_notice", group_id=group_id)
                    logger.debug(f"API å“åº” (_get_group_notice): {json.dumps(raw_notices, ensure_ascii=False)}")
                    
                    # ç²¾ç®€å…¬å‘Šä¿¡æ¯
                    for n in raw_notices:
                        notices.append({
                            "notice_id": n.get("notice_id"),
                            "sender_id": n.get("sender_id"),
                            "publish_time": n.get("publish_time"),
                            "text": n.get("message", {}).get("text", "")
                        })
                    
                    # å¢é‡å¯¹æ¯”ç¾¤å…¬å‘Š
                    if latest_data and "notices" in latest_data:
                        old_notices_map = {n["notice_id"]: n for n in latest_data["notices"]}
                        new_notices_map = {n["notice_id"]: n for n in notices}
                        
                        # æ£€æµ‹æ–°å¢
                        joiners = [n for nid, n in new_notices_map.items() if nid not in old_notices_map]
                        if joiners:
                            for n in joiners:
                                self._append_log(group_id, "content_changes", {"type": "æ–°å¢å…¬å‘Š", "notice_id": n["notice_id"], "text": n["text"]})
                        
                        # æ£€æµ‹åˆ é™¤
                        deleted_notices = [n for nid, n in old_notices_map.items() if nid not in new_notices_map]
                        if deleted_notices:
                            logger.info(f"æ£€æµ‹åˆ°å·²åˆ é™¤çš„ç¾¤å…¬å‘Š: {deleted_notices}")
                            self._archive_deleted_items(group_id, "notices", deleted_notices)
                            for n in deleted_notices:
                                self._append_log(group_id, "content_changes", {"type": "å…¬å‘Šå·²åˆ é™¤", "notice_id": n["notice_id"]})
                        
                        # æ£€æµ‹ç¼–è¾‘ (ID ç›¸åŒä½†å†…å®¹æˆ–å…¶ä»–å±æ€§å˜åŒ–)
                        for nid, new_n in new_notices_map.items():
                            if nid in old_notices_map:
                                old_n = old_notices_map[nid]
                                # åªè¦ text å˜åŒ–äº†ï¼Œå°±è®¤ä¸ºè¢«ç¼–è¾‘äº†
                                if new_n.get("text") != old_n.get("text"):
                                    logger.info(f"æ£€æµ‹åˆ°å…¬å‘Šå·²ç¼–è¾‘ (ID: {nid})")
                                    self._append_log(group_id, "content_changes", {
                                        "type": "å…¬å‘Šå·²ç¼–è¾‘", 
                                        "notice_id": nid,
                                        "old_text": old_n.get("text"),
                                        "new_text": new_n.get("text")
                                    })
                except Exception as e:
                    logger.warning(f"è·å–ç¾¤å…¬å‘Šå¤±è´¥: {e}")
                
            # 4. è·å–ç²¾åæ¶ˆæ¯
            essence = []
            if "ç²¾åæ¶ˆæ¯" in self.backup_options:
                try:
                    raw_essence = await client.get_essence_msg_list(group_id=group_id)
                    logger.debug(f"API å“åº” (get_essence_msg_list): {json.dumps(raw_essence, ensure_ascii=False)}")
                    
                    # ç²¾ç®€ç²¾åæ¶ˆæ¯
                    for e in raw_essence:
                        essence.append({
                            "message_id": e.get("message_id"),
                            "sender_id": e.get("sender_id"),
                            "sender_nick": e.get("sender_nick"),
                            "operator_id": e.get("operator_id"),
                            "operator_nick": e.get("operator_nick"),
                            "operator_time": e.get("operator_time"),
                            "content": e.get("content")
                        })
                    
                    # å¢é‡å¯¹æ¯”ç²¾åæ¶ˆæ¯
                    if latest_data and "essence" in latest_data:
                        old_essence_map = {e["message_id"]: e for e in latest_data["essence"]}
                        new_essence_map = {e["message_id"]: e for e in essence}
                        
                        deleted_essence = [e for mid, e in old_essence_map.items() if mid not in new_essence_map]
                        if deleted_essence:
                            logger.info(f"æ£€æµ‹åˆ°å·²åˆ é™¤çš„ç²¾åæ¶ˆæ¯: {deleted_essence}")
                            self._archive_deleted_items(group_id, "essence", deleted_essence)
                            for e in deleted_essence:
                                self._append_log(group_id, "content_changes", {"type": "ç²¾åæ¶ˆæ¯å·²åˆ é™¤", "message_id": e["message_id"]})
                except Exception as e:
                    logger.warning(f"è·å–ç²¾åæ¶ˆæ¯å¤±è´¥: {e}")
                
            # 5. è·å–ç¾¤è£èª‰
            honors = {}
            if "ç¾¤è£èª‰" in self.backup_options:
                try:
                    honors = await client.get_group_honor_info(group_id=group_id)
                    logger.debug(f"API å“åº” (get_group_honor_info): {json.dumps(honors, ensure_ascii=False)}")
                except Exception as e:
                    logger.warning(f"è·å–ç¾¤è£èª‰å¤±è´¥: {e}")

            # 6. è·å–ç¾¤ç›¸å†Œå¹¶å¤‡ä»½åŸå›¾
            albums = []
            album_media_map = {}
            if "ç¾¤ç›¸å†Œ" in self.backup_options:
                try:
                    raw_albums = await client.api.call_action("get_qun_album_list", group_id=str(group_id))
                    logger.debug(f"API å“åº” (get_qun_album_list): {json.dumps(raw_albums, ensure_ascii=False)}")
                    if raw_albums:
                        logger.info(f"å‘ç° {len(raw_albums)} ä¸ªç›¸å†Œï¼Œæ­£åœ¨å¤‡ä»½åŸå›¾...")
                        for album in raw_albums:
                            album_id = album.get("album_id")
                            album_name = album.get("name", album_id)
                            
                            albums.append({
                                "album_id": album_id,
                                "name": album_name,
                                "create_time": album.get("create_time"),
                                "modify_time": album.get("modify_time"),
                                "creator_nick": album.get("creator", {}).get("nick"),
                                "upload_number": album.get("upload_number")
                            })
                            
                            # å¤„ç†ç›¸å†Œæ”¹å
                            if latest_data and "albums" in latest_data:
                                old_albums = {a["album_id"]: a.get("name") for a in latest_data["albums"]}
                                if album_id in old_albums and old_albums[album_id] != album_name:
                                    old_name = old_albums[album_id]
                                    if old_name:
                                        old_path = Path(self.plugin_data_dir) / str(group_id) / "albums" / old_name
                                        new_path = Path(self.plugin_data_dir) / str(group_id) / "albums" / album_name
                                        if old_path.exists() and not new_path.exists():
                                            logger.info(f"æ£€æµ‹åˆ°ç›¸å†Œæ”¹å: {old_name} -> {album_name}ã€‚æ­£åœ¨é‡å‘½åæ–‡ä»¶å¤¹ã€‚")
                                            import shutil
                                            try:
                                                shutil.move(str(old_path), str(new_path))
                                                self._append_log(group_id, "content_changes", {
                                                    "type": "ç›¸å†Œå·²æ”¹å",
                                                    "album_id": album_id,
                                                    "old_name": old_name,
                                                    "new_name": album_name
                                                })
                                            except Exception as e:
                                                logger.error(f"é‡å‘½åç›¸å†Œæ–‡ä»¶å¤¹å¤±è´¥: {e}")
                            
                            # æ£€æŸ¥ç›¸å†Œæ˜¯å¦æœ‰æ›´æ–°ï¼ˆé€šè¿‡ä¿®æ”¹æ—¶é—´ï¼‰
                            media_list = []
                            is_album_updated = True
                            if latest_data and "albums" in latest_data and "album_media" in latest_data:
                                old_album_info = next((a for a in latest_data["albums"] if a["album_id"] == album_id), None)
                                if old_album_info and str(old_album_info.get("modify_time")) == str(album.get("modify_time")):
                                    # ä¿®æ”¹æ—¶é—´æœªå˜ï¼Œå°è¯•å¤ç”¨ä¸Šæ¬¡çš„åª’ä½“åˆ—è¡¨
                                    media_list = latest_data["album_media"].get(album_id, [])
                                    if media_list:
                                        is_album_updated = False
                                        logger.info(f"ç›¸å†Œ {album_name} ä¿®æ”¹æ—¶é—´æœªå˜ï¼Œè·³è¿‡ API è¯·æ±‚ï¼Œå¤ç”¨ä¸Šæ¬¡å¤‡ä»½çš„ {len(media_list)} ä¸ªåª’ä½“è®°å½•ã€‚")
                            
                            if is_album_updated:
                                try:
                                    # æ ¹æ® log.txt æ ¼å¼ï¼ŒAPI è¿”å›ä¸€ä¸ªåŒ…å« "media_list" çš„å¯¹è±¡
                                    result = await client.api.call_action("get_group_album_media_list", group_id=str(group_id), album_id=album_id)
                                    logger.debug(f"è·å–ç›¸å†Œ {album_name}({album_id}) åª’ä½“åˆ—è¡¨ç»“æœ: {json.dumps(result, ensure_ascii=False)}")
                                    
                                    raw_media_list = []
                                    if isinstance(result, dict):
                                        # æ£€æŸ¥æ—¥å¿—ä¸­çš„ media_list é”®
                                        if "media_list" in result:
                                            raw_media_list = result["media_list"]
                                        elif "media" in result:
                                            raw_media_list = result["media"]
                                        elif "m_media" in result:
                                            raw_media_list = result["m_media"]
                                        elif "album" in result:
                                            album_info = result["album"]
                                            if "cover" in album_info and "image" in album_info["cover"]:
                                                raw_media_list = [album_info["cover"]]
                                    elif isinstance(result, list):
                                        raw_media_list = result
                                    
                                    for m in raw_media_list:
                                        # æå–åª’ä½“è¯¦æƒ…ï¼šlog æ˜¾ç¤ºåª’ä½“é¡¹åŒ…å« "image" æˆ– "video"
                                        media_detail = m.get("image") or m.get("video") or m
                                        
                                        # æå– URLï¼šURL åœ¨ photo_url åˆ—è¡¨ä¸­
                                        photo_urls = media_detail.get("photo_url", [])
                                        # ä¼˜å…ˆé€‰æ‹© spec 1 (åŸå›¾) æˆ– 6 (é«˜æ¸…)ï¼Œå¦‚æœæ²¡æœ‰åˆ™å–ç¬¬ä¸€ä¸ª
                                        best_url = ""
                                        if photo_urls:
                                            # å°è¯•å¯»æ‰¾ spec 1 æˆ– 6
                                            for p in photo_urls:
                                                if p.get("spec") in [1, 6]:
                                                    best_url = p.get("url", {}).get("url", "")
                                                    break
                                            if not best_url:
                                                best_url = photo_urls[0].get("url", {}).get("url", "")
                                        
                                        # å¦‚æœæ²¡æœ‰ photo_urlï¼Œå°è¯• default_url
                                        if not best_url:
                                            best_url = media_detail.get("default_url", {}).get("url", "")

                                        media_list.append({
                                            "media_id": media_detail.get("lloc") or m.get("media_id") or m.get("id"),
                                            "url": best_url,
                                            "media_type": m.get("type") or m.get("media_type"),
                                            "upload_time": m.get("upload_time")
                                        })
                                except Exception as e:
                                    logger.error(f"è·å–ç›¸å†Œ {album_id} åª’ä½“åˆ—è¡¨å¤±è´¥: {e}")
                            
                            album_media_map[album_id] = media_list
                            
                            # ä¸‹è½½åŸå›¾ï¼ˆå¦‚æœæ˜¯æ–°ç›¸å†Œã€å·²æ›´æ–°æˆ–æœ¬åœ°æ–‡ä»¶ç¼ºå¤±ï¼Œ_download_file ä¼šå¤„ç†ï¼‰
                            if media_list:
                                if is_album_updated:
                                    logger.info(f"æ­£åœ¨ä¸‹è½½ç›¸å†Œ {album_name} ä¸­çš„ {len(media_list)} ä¸ªåª’ä½“æ–‡ä»¶...")
                                download_tasks = []
                                album_save_dir = Path(self.plugin_data_dir) / str(group_id) / "albums" / album_name
                                album_save_dir.mkdir(parents=True, exist_ok=True)
                                
                                for media in media_list:
                                    url = media.get("url")
                                    if url:
                                        media_id = media.get("media_id")
                                        if not media_id: continue
                                        
                                        file_ext = ".jpg" 
                                        media_type = str(media.get("media_type", "")).lower()
                                        if "video" in media_type or media_type == "2":
                                            file_ext = ".mp4"
                                        
                                        save_path = album_save_dir / f"{media_id}{file_ext}"
                                        download_tasks.append(self._download_file(url, save_path))
                                
                                if download_tasks:
                                    await asyncio.gather(*download_tasks)
                except Exception as e:
                    logger.error(f"å¤‡ä»½ç¾¤ç›¸å†Œå¤±è´¥: {e}")

            # 7. å¢é‡å¯¹æ¯”ç¾¤ç›¸å†Œï¼ˆå¤„ç†å·²åˆ é™¤çš„å›¾ç‰‡/ç›¸å†Œï¼‰
            if "ç¾¤ç›¸å†Œ" in self.backup_options and latest_data and "album_media" in latest_data:
                try:
                    old_album_media = latest_data["album_media"]
                    # æŸ¥æ‰¾å·²åˆ é™¤çš„ç›¸å†Œ
                    for old_album_id, old_media_list in old_album_media.items():
                        if old_album_id not in album_media_map:
                            # æ•´ä¸ªç›¸å†Œè¢«åˆ äº†
                            self._append_log(group_id, "content_changes", {"type": "ç›¸å†Œå·²åˆ é™¤", "album_id": old_album_id})
                            old_albums = {a["album_id"]: a["name"] for a in latest_data.get("albums", [])}
                            old_name = old_albums.get(old_album_id)
                            if old_name:
                                src_dir = Path(self.plugin_data_dir) / str(group_id) / "albums" / old_name
                                if src_dir.exists():
                                    dst_dir = Path(self.plugin_data_dir) / str(group_id) / "logs" / "deleted_items" / "albums" / old_name
                                    dst_dir.parent.mkdir(parents=True, exist_ok=True)
                                    import shutil
                                    if dst_dir.exists(): shutil.rmtree(dst_dir)
                                    logger.info(f"æ­£åœ¨å°†å·²åˆ é™¤çš„ç›¸å†Œç›®å½•ä» {src_dir} ç§»åŠ¨åˆ° {dst_dir}")
                                    shutil.move(str(src_dir), str(dst_dir))
                        else:
                            # ç›¸å†Œè¿˜åœ¨ï¼Œæ£€æŸ¥é‡Œé¢çš„å›¾ç‰‡æœ‰æ²¡æœ‰è¢«åˆ 
                            new_media_ids = {m["media_id"] for m in album_media_map[old_album_id]}
                            deleted_media = [m for m in old_media_list if m["media_id"] not in new_media_ids]
                            
                            if deleted_media:
                                # è®°å½•æ—¥å¿—
                                for m in deleted_media:
                                    self._append_log(group_id, "content_changes", {
                                        "type": "åª’ä½“æ–‡ä»¶å·²åˆ é™¤", 
                                        "album_id": old_album_id, 
                                        "media_id": m["media_id"]
                                    })
                                
                                # å°†è¢«åˆ å›¾ç‰‡ç§»åŠ¨åˆ°å›æ”¶ç«™
                                # æ‰¾åˆ°å½“å‰ç›¸å†Œçš„æ–‡ä»¶å¤¹å
                                current_albums = {a["album_id"]: a["name"] for a in albums}
                                album_name = current_albums.get(old_album_id)
                                if album_name:
                                    for m in deleted_media:
                                        # å°è¯•ä¸åŒçš„å¯èƒ½åç¼€
                                        for ext in [".jpg", ".mp4", ".png"]:
                                            src_file = Path(self.plugin_data_dir) / str(group_id) / "albums" / album_name / f"{m['media_id']}{ext}"
                                            if src_file.exists():
                                                dst_file = Path(self.plugin_data_dir) / str(group_id) / "logs" / "deleted_items" / "albums" / album_name / f"{m['media_id']}{ext}"
                                                dst_file.parent.mkdir(parents=True, exist_ok=True)
                                                logger.info(f"æ­£åœ¨å°†å·²åˆ é™¤çš„åª’ä½“æ–‡ä»¶ä» {src_file} ç§»åŠ¨åˆ° {dst_file}")
                                                import shutil
                                                shutil.move(str(src_file), str(dst_file))
                                                break
                except Exception as e:
                    logger.error(f"å¤„ç†å·²åˆ é™¤ç›¸å†Œå›¾ç‰‡æ—¶å‡ºé”™: {e}")

            # å‡†å¤‡æ•°æ®ç›®å½•
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            group_base_dir = Path(self.plugin_data_dir) / str(group_id)
            backup_path = group_base_dir / timestamp
            backup_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ JSON
            data_to_save = {
                "group_detail": group_detail,
                "members": members,
                "notices": notices,
                "essence": essence,
                "honors": honors,
                "albums": albums,
                "album_media": album_media_map
            }
            
            # æ‰§è¡Œä¿å­˜
            for key, val in data_to_save.items():
                file_name = f"{key}.json"
                save_file_path = backup_path / file_name
                with open(save_file_path, "w", encoding="utf-8") as f:
                    json.dump(val, f, ensure_ascii=False, indent=4)
                logger.info(f"æˆåŠŸä¿å­˜å¤‡ä»½å¿«ç…§æ–‡ä»¶: {save_file_path}")
            
            # åˆ é™¤é™¤å½“å‰åˆšåˆ›å»ºçš„å¤‡ä»½ä»¥å¤–çš„æ‰€æœ‰æ—§å¿«ç…§æ–‡ä»¶å¤¹
            all_backups = sorted([d for d in group_base_dir.iterdir() if d.is_dir() and d.name.replace("_", "").isdigit()], key=lambda x: x.name)
            for old_backup in all_backups:
                if old_backup.name != timestamp:
                    try:
                        import shutil
                        shutil.rmtree(str(old_backup))
                        logger.info(f"å·²æ¸…ç†æ—§å¤‡ä»½å¿«ç…§: {old_backup.name}")
                    except Exception as e:
                        logger.error(f"æ¸…ç†æ—§å¤‡ä»½å¤±è´¥ {old_backup.name}: {e}")
            
            yield event.plain_result(f"âœ… ç¾¤ {group_id} å¤‡ä»½æˆåŠŸï¼\nï¼ˆå·²æ ¹æ®æœ€æ–°æ•°æ®æ›´æ–°å¿«ç…§ï¼Œå¹¶ä¿ç•™å†å²å˜æ›´æ—¥å¿—ï¼‰")
            
        except Exception as e:
            logger.error(f"ç¾¤å¤‡ä»½å‡ºé”™: {e}")
            yield event.plain_result(f"âŒ å¤‡ä»½å¤±è´¥: {e}")

    @filter.command("åˆ é™¤ç¾¤å¤‡ä»½")
    async def delete_group_backup(self, event: AstrMessageEvent, group_id_arg: str = ""):
        """åˆ é™¤ç¾¤å¤‡ä»½ [ç¾¤å·]ï¼šç‰©ç†åˆ é™¤æŒ‡å®šç¾¤ç»„çš„æ‰€æœ‰å¤‡ä»½æ•°æ®"""
        # æƒé™æ£€æŸ¥
        is_admin = event.is_admin()
        user_id = int(event.get_sender_id())
        if not is_admin and (not self.admin_users or user_id not in self.admin_users):
            yield event.plain_result(f"âŒ æ­¤æŒ‡ä»¤ä»…é™ç®¡ç†å‘˜ä½¿ç”¨")
            return

        target_group_id = group_id_arg.strip()
        if not target_group_id:
            target_group_id = event.get_group_id()
        
        if not target_group_id:
            yield event.plain_result("è¯·åœ¨ç¾¤èŠä¸­ä½¿ç”¨æ­¤æŒ‡ä»¤ï¼Œæˆ–åœ¨æŒ‡ä»¤åè·Ÿéšç¾¤å·ã€‚")
            return

        try:
            group_id = int(target_group_id)
            group_dir = Path(self.plugin_data_dir) / str(group_id)
            
            if not group_dir.exists():
                yield event.plain_result(f"ğŸ” æœªæ‰¾åˆ°ç¾¤ {group_id} çš„å¤‡ä»½æ•°æ®ã€‚")
                return

            import shutil
            # ç‰©ç†åˆ é™¤æ•´ä¸ªç¾¤ç›®å½•
            shutil.rmtree(str(group_dir))
            
            logger.info(f"ç®¡ç†å‘˜ {user_id} åˆ é™¤äº†ç¾¤ {group_id} çš„æ‰€æœ‰å¤‡ä»½æ•°æ®ã€‚")
            yield event.plain_result(f"âœ… å·²æˆåŠŸåˆ é™¤ç¾¤ {group_id} çš„æ‰€æœ‰å¤‡ä»½æ•°æ®ï¼ˆåŒ…æ‹¬ç›¸å†Œå’Œæ—¥å¿—ï¼‰ã€‚")

        except Exception as e:
            logger.error(f"åˆ é™¤ç¾¤å¤‡ä»½å‡ºé”™: {e}")
            yield event.plain_result(f"âŒ åˆ é™¤å¤±è´¥: {e}")

    @filter.command("ç¾¤å¯¼å‡º")
    async def group_export(self, event: AstrMessageEvent, group_id_arg: str = ""):
        """ç¾¤å¯¼å‡º [ç¾¤å·]ï¼šå¯¼å‡ºå½“å‰ç¾¤æˆ–æŒ‡å®šç¾¤æ•°æ®ä¸º Excel å¹¶å‘é€"""
        # æƒé™æ£€æŸ¥ï¼šBot ç®¡ç†å‘˜ æˆ– é…ç½®é¡¹ä¸­çš„ç®¡ç†å‘˜
        is_admin = event.is_admin()
        user_id = int(event.get_sender_id())
        if not is_admin and (not self.admin_users or user_id not in self.admin_users):
            yield event.plain_result(f"âŒ æ­¤æŒ‡ä»¤ä»…é™ç®¡ç†å‘˜ä½¿ç”¨")
            return

        target_group_id = group_id_arg.strip()
        if not target_group_id:
            target_group_id = event.get_group_id()
        
        if not target_group_id:
            yield event.plain_result("è¯·åœ¨ç¾¤èŠä¸­ä½¿ç”¨æ­¤æŒ‡ä»¤ï¼Œæˆ–åœ¨æŒ‡ä»¤åè·Ÿéšç¾¤å·ã€‚")
            return

        try:
            group_id = int(target_group_id)
            client = event.bot
            
            yield event.plain_result(f"æ­£åœ¨å¯¼å‡ºç¾¤ {group_id} çš„æ•°æ®...")

            output_buffer = BytesIO()
            with pd.ExcelWriter(output_buffer, engine="openpyxl") as writer:
                # 1. å¯¼å‡ºç¾¤æ¦‚å†µ (ç¾¤ä¿¡æ¯)
                if "ç¾¤ä¿¡æ¯" in self.backup_options:
                    try:
                        detail = await client.api.call_action("get_group_detail_info", group_id=group_id)
                        if detail:
                            # æ˜ å°„å¸¸ç”¨å­—æ®µä¸ºä¸­æ–‡
                            display_detail = {
                                "ç¾¤åç§°": detail.get("groupName"),
                                "ç¾¤å·": detail.get("groupCode"),
                                "ç¾¤åˆ†ç±»": detail.get("groupClassText"),
                                "ç¾¤ä¸»QQ": detail.get("ownerUin"),
                                "æˆå‘˜äººæ•°": detail.get("memberNum"),
                                "æœ€å¤§äººæ•°": detail.get("maxMemberNum"),
                                "å½“å‰æ´»è·ƒäººæ•°": detail.get("activeMemberNum"),
                                "ç¾¤å…¬å‘Š": detail.get("groupMemo"),
                            }
                            # å°†å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼ä»¥ä¾¿å¯¼å‡º
                            detail_list = [{"å±æ€§": k, "å€¼": v} for k, v in display_detail.items() if v is not None]
                            
                            pd.DataFrame(detail_list).to_excel(writer, index=False, sheet_name="ç¾¤æ¦‚å†µ")
                    except Exception as e:
                        logger.warning(f"å¯¼å‡ºç¾¤æ¦‚å†µå¤±è´¥: {e}")

                # 2. å¯¼å‡ºç¾¤æˆå‘˜
                if "ç¾¤æˆå‘˜" in self.backup_options:
                    try:
                        members = await client.get_group_member_list(group_id=group_id)
                        processed_members = []
                        for m in members:
                            item = {}
                            for opt, api_key in self.field_map.items():
                                val = m.get(api_key, "")
                                if api_key in ["join_time", "last_sent_time"]:
                                    val = self._format_timestamp(val)
                                elif api_key == "role":
                                    val = {"owner": "ç¾¤ä¸»", "admin": "ç®¡ç†å‘˜", "member": "æˆå‘˜"}.get(val, val)
                                item[opt] = val
                            processed_members.append(item)
                        pd.DataFrame(processed_members).to_excel(writer, index=False, sheet_name="ç¾¤æˆå‘˜")
                    except Exception as e:
                        logger.warning(f"å¯¼å‡ºç¾¤æˆå‘˜å¤±è´¥: {e}")

                # 3. å¯¼å‡ºç¾¤å…¬å‘Š
                if "ç¾¤å…¬å‘Š" in self.backup_options:
                    try:
                        notices = await client.api.call_action("_get_group_notice", group_id=group_id)
                        if notices:
                            processed_notices = []
                            for n in notices:
                                msg = n.get("message", {})
                                content = msg.get("text", "")
                                processed_notices.append({
                                    "å‘å¸ƒè€…": n.get("sender_id"),
                                    "å‘å¸ƒæ—¶é—´": self._format_timestamp(n.get("publish_time")),
                                    "å†…å®¹": content
                                })
                            pd.DataFrame(processed_notices).to_excel(writer, index=False, sheet_name="ç¾¤å…¬å‘Š")
                    except Exception as e:
                        logger.warning(f"å¯¼å‡ºç¾¤å…¬å‘Šå¤±è´¥: {e}")

                # 4. å¯¼å‡ºç²¾åæ¶ˆæ¯
                if "ç²¾åæ¶ˆæ¯" in self.backup_options:
                    try:
                        essence = await client.get_essence_msg_list(group_id=group_id)
                        if essence:
                            processed_essence = []
                            for e in essence:
                                raw_content = e.get("content", [])
                                content_str = ""
                                if isinstance(raw_content, list):
                                    for seg in raw_content:
                                        if seg.get("type") == "text":
                                            content_str += seg.get("data", {}).get("text", "")
                                        elif seg.get("type") == "at":
                                            content_str += f"@{seg.get('data', {}).get('qq', '')} "
                                        else:
                                            content_str += f"[{seg.get('type')}]"
                                else:
                                    content_str = str(raw_content)

                                processed_essence.append({
                                    "å‘é€è€…": e.get("sender_id"),
                                    "å‘é€æ—¶é—´": self._format_timestamp(e.get("operator_time")), # log æ˜¾ç¤ºæ˜¯ operator_time
                                    "å†…å®¹": content_str,
                                    "æ“ä½œè€…": e.get("operator_id")
                                })
                            pd.DataFrame(processed_essence).to_excel(writer, index=False, sheet_name="ç²¾åæ¶ˆæ¯")
                    except Exception as e:
                        logger.warning(f"å¯¼å‡ºç²¾åæ¶ˆæ¯å¤±è´¥: {e}")

                # 5. å¯¼å‡ºç¾¤è£èª‰
                if "ç¾¤è£èª‰" in self.backup_options:
                    try:
                        honors = await client.get_group_honor_info(group_id=group_id)
                        if honors:
                            honor_list = []
                            # å¤„ç†é¾™ç‹ç­‰è£èª‰
                            for honor_type, honor_data in honors.items():
                                if honor_type == "group_id": continue
                                if isinstance(honor_data, dict) and "user_id" in honor_data:
                                    honor_list.append({"è£èª‰ç±»å‹": honor_type, "QQå·": honor_data.get("user_id"), "æè¿°": honor_data.get("nickname")})
                                elif isinstance(honor_data, list):
                                    for h in honor_data:
                                        honor_list.append({"è£èª‰ç±»å‹": honor_type, "QQå·": h.get("user_id"), "æè¿°": h.get("nickname")})
                            if honor_list:
                                pd.DataFrame(honor_list).to_excel(writer, index=False, sheet_name="ç¾¤è£èª‰")
                    except Exception as e:
                        logger.warning(f"å¯¼å‡ºç¾¤è£èª‰å¤±è´¥: {e}")

                # 6. å¯¼å‡ºç¾¤ç›¸å†Œåˆ—è¡¨
                if "ç¾¤ç›¸å†Œ" in self.backup_options:
                    try:
                        albums = await client.api.call_action("get_qun_album_list", group_id=str(group_id))
                        if albums:
                            processed_albums = []
                            for a in albums:
                                processed_albums.append({
                                    "ç›¸å†Œå": a.get("name"),
                                    "å›¾ç‰‡æ•°é‡": a.get("upload_number"),
                                    "åˆ›å»ºè€…": a.get("creator", {}).get("nick"),
                                    "åˆ›å»ºæ—¶é—´": self._format_timestamp(a.get("create_time")),
                                    "ä¿®æ”¹æ—¶é—´": self._format_timestamp(a.get("modify_time"))
                                })
                            pd.DataFrame(processed_albums).to_excel(writer, index=False, sheet_name="ç¾¤ç›¸å†Œåˆ—è¡¨")
                    except Exception as e:
                        logger.warning(f"å¯¼å‡ºç¾¤ç›¸å†Œå¤±è´¥: {e}")
            
            file_content = output_buffer.getvalue()
            if not file_content:
                yield event.plain_result("âŒ æœªèƒ½è·å–åˆ°ä»»ä½•æ•°æ®è¿›è¡Œå¯¼å‡ºã€‚")
                return

            file_name = f"ç¾¤{group_id}_å…¨æ•°æ®å¯¼å‡º_{datetime.now().strftime('%Y%m%d')}.xlsx"
            file_content_base64 = base64.b64encode(file_content).decode("utf-8")
            
            # ç¡®å®šå‘é€ç›®æ ‡
            if event.message_obj.type == MessageType.GROUP_MESSAGE:
                await client.upload_group_file(
                    group_id=int(event.get_group_id()),
                    file=f"base64://{file_content_base64}",
                    name=file_name
                )
            else:
                await client.upload_private_file(
                    user_id=int(event.get_sender_id()),
                    file=f"base64://{file_content_base64}",
                    name=file_name
                )
            
            yield event.plain_result(f"âœ… ç¾¤ {group_id} æ•°æ®å¯¼å‡ºæˆåŠŸï¼Œæ–‡ä»¶å·²ä¸Šä¼ ã€‚")

        except Exception as e:
            logger.error(f"ç¾¤å¯¼å‡ºå‡ºé”™: {e}")
            yield event.plain_result(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
