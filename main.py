import os
import json
import asyncio
import pandas as pd
import aiohttp
from datetime import datetime
from io import BytesIO
from pathlib import Path
from typing import List, Dict, Any, Optional
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, StarTools, register
from astrbot.api import logger
from astrbot.core.platform.message_type import MessageType
import base64
import zipfile
import shutil

@register(
    "astrbot_plugin_group_backup",
    "Foolllll",
    "ç¾¤å¤‡ä»½æ’ä»¶ï¼Œå¤‡ä»½ç¾¤æˆå‘˜ã€å…¬å‘Šã€ç²¾åç­‰æ•°æ®",
    "0.1",
    "https://github.com/Foolllll-J/astrbot_plugin_group_backup"
)
class GroupBackupPlugin(Star):
    def __init__(self, context: Context, config: Optional[Dict] = None):
        super().__init__(context)
        self.config = config if config else {}
        self.plugin_data_dir = StarTools.get_data_dir("astrbot_plugin_group_backup")
        self.download_semaphore = asyncio.Semaphore(5) # é™åˆ¶å¹¶å‘ä¸‹è½½æ•°
        
        # å­—æ®µæ˜ å°„ï¼šé…ç½®é¡¹å -> API è¿”å›çš„é”®å
        self.field_map = {
            "QQå·": "user_id",
            "æ˜µç§°": "nickname",
            "ç¾¤æ˜µç§°": "card",
            "æƒé™": "role",
            "ç­‰çº§": "level",
            "å¤´è¡”": "title",
            "åŠ ç¾¤æ—¶é—´": "join_time",
            "æœ€åå‘è¨€": "last_sent_time",
        }

    @property
    def admin_users(self) -> List[int]:
        return [int(u) for u in self.config.get("admin_users", [])]

    @property
    def backup_options(self) -> List[str]:
        return self.config.get("backup_options", ["ç¾¤ä¿¡æ¯", "ç¾¤å¤´åƒ", "ç¾¤æˆå‘˜", "ç¾¤å…¬å‘Š", "ç¾¤ç²¾å", "ç¾¤ç›¸å†Œ", "ç¾¤è£èª‰"])

    @property
    def restore_options(self) -> List[str]:
        return self.config.get("restore_options", ["ç¾¤åç§°", "ç¾¤å¤´åƒ", "ç¾¤æ˜µç§°", "ç¾¤å¤´è¡”", "ç¾¤ç®¡ç†", "ç¾¤ç›¸å†Œ"])

    def _format_timestamp(self, timestamp):
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        if isinstance(timestamp, (int, float)) and timestamp > 0:
            return datetime.fromtimestamp(float(timestamp)).strftime("%Y-%m-%d %H:%M:%S")
        return "æœªçŸ¥"

    def _format_essence_content(self, raw_content):
        """æ ¼å¼åŒ–ç²¾åæ¶ˆæ¯å†…å®¹"""
        content_str = ""
        if isinstance(raw_content, list):
            for seg in raw_content:
                if seg.get("type") == "text":
                    content_str += seg.get("data", {}).get("text", "")
                elif seg.get("type") == "at":
                    content_str += f"@{seg.get('data', {}).get('qq', '')} "
                elif seg.get("type") == "image":
                    content_str += "[å›¾ç‰‡]"
                elif seg.get("type") == "face":
                    content_str += "[è¡¨æƒ…]"
                else:
                    content_str += f"[{seg.get('type', 'æœªçŸ¥')}]"
        else:
            content_str = str(raw_content)
        return content_str

    async def _download_file(self, url: str, save_path: Path, overwrite: bool = False):
        """ä¸‹è½½æ–‡ä»¶ï¼Œå¦‚æœå·²å­˜åœ¨ä¸”æœªå¼€å¯ overwrite åˆ™è·³è¿‡"""
        if not overwrite and save_path.exists():
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {save_path}")
            return True
        
        async with self.download_semaphore:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, timeout=300) as response:
                        if response.status == 200:
                            content = await response.read()
                            
                            # å¦‚æœæ˜¯è¦†ç›–æ¨¡å¼ï¼Œä¸”æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
                            if overwrite and save_path.exists():
                                import hashlib
                                with open(save_path, "rb") as f:
                                    old_content = f.read()
                                if hashlib.md5(content).hexdigest() == hashlib.md5(old_content).hexdigest():
                                    return False # å†…å®¹æ— å˜åŒ–
                            
                            save_path.parent.mkdir(parents=True, exist_ok=True)
                            with open(save_path, "wb") as f:
                                f.write(content)
                            logger.info(f"æˆåŠŸä¿å­˜æ–‡ä»¶: {save_path}")
                            return True # å†…å®¹æœ‰å˜åŒ–æˆ–æ–°ä¸‹è½½
                        else:
                            logger.warning(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥ {url}: HTTP {response.status}")
            except Exception as e:
                logger.error(f"ä¸‹è½½è¿‡ç¨‹å‡ºé”™ {url}: {e}")
        return False

    def _get_latest_backup_data(self, group_id: int) -> Dict[str, Any]:
        """è·å–æœ€è¿‘ä¸€æ¬¡å¤‡ä»½çš„æ•°æ®"""
        group_dir = Path(self.plugin_data_dir) / str(group_id)
        if not group_dir.exists():
            return {}
        
        # æŸ¥æ‰¾æ—¶é—´æˆ³ç›®å½•
        backups = [d for d in group_dir.iterdir() if d.is_dir() and d.name.replace("_", "").isdigit()]
        if not backups:
            return {}
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        latest_backup_dir = sorted(backups, key=lambda x: x.name)[-1]
        
        data = {}
        for file_path in latest_backup_dir.glob("*.json"):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = json.load(f)
                    data[file_path.stem] = content
            except Exception as e:
                logger.warning(f"åŠ è½½ä¸Šä¸€æ¬¡å¤‡ä»½æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        
        return data

    def _append_log(self, group_id: int, log_name: str, log_entry: Dict[str, Any]):
        """è¿½åŠ æ—¥å¿—è®°å½•"""
        log_dir = Path(self.plugin_data_dir) / str(group_id) / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = log_dir / f"{log_name}.json"
        
        logs = []
        if log_file.exists():
            try:
                with open(log_file, "r", encoding="utf-8") as f:
                    logs = json.load(f)
            except:
                logs = []
        
        logs.append({
            "log_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            **log_entry
        })
        
        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(logs, f, ensure_ascii=False, indent=4)
        logger.info(f"å·²è¿½åŠ æ—¥å¿—åˆ° {log_file}: {log_entry}")

    def _archive_deleted_items(self, group_id: int, item_type: str, items: List[Any]):
        """å½’æ¡£å·²åˆ é™¤çš„é¡¹ç›®åˆ°å›æ”¶ç«™"""
        archive_dir = Path(self.plugin_data_dir) / str(group_id) / "logs"
        archive_dir.mkdir(parents=True, exist_ok=True)
        archive_file = archive_dir / "deleted_items.json"
        
        archive = {}
        if archive_file.exists():
            try:
                with open(archive_file, "r", encoding="utf-8") as f:
                    archive = json.load(f)
            except:
                archive = {}
        
        if item_type not in archive:
            archive[item_type] = []
            
        for item in items:
            archive[item_type].append({
                "deleted_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "content": item
            })
            
        with open(archive_file, "w", encoding="utf-8") as f:
            json.dump(archive, f, ensure_ascii=False, indent=4)
        logger.info(f"å·²å½’æ¡£ {len(items)} ä¸ªå·²åˆ é™¤çš„é¡¹ç›®ï¼ˆç±»å‹: '{item_type}'ï¼‰åˆ° {archive_file}")

    async def _backup_albums(self, client, group_id: int, latest_data: Dict = None):
        """å¤‡ä»½ç¾¤ç›¸å†Œï¼Œè¿”å› (albums_list, album_media_map)"""
        albums = []
        album_media_map = {}
        try:
            raw_albums = await client.get_qun_album_list(group_id=str(group_id))
            if isinstance(raw_albums, dict) and raw_albums.get("retcode", 0) != 0:
                raise Exception(f"API å“åº”å¼‚å¸¸: {raw_albums}")
            logger.debug(f"API å“åº” (get_qun_album_list): {json.dumps(raw_albums, ensure_ascii=False)}")
            if raw_albums:
                logger.info(f"å‘ç° {len(raw_albums)} ä¸ªç›¸å†Œï¼Œæ­£åœ¨å¤‡ä»½åŸå›¾...")
                for album in raw_albums:
                    album_id = album.get("album_id")
                    album_name = album.get("name", album_id)
                    
                    # ç²¾ç®€ç›¸å†Œä¿¡æ¯
                    albums.append({
                        "album_id": album_id,
                        "name": album_name,
                        "create_time": album.get("create_time"),
                        "modify_time": album.get("modify_time"),
                        "creator_nick": album.get("creator", {}).get("nick"),
                        "upload_number": album.get("upload_number")
                    })
                    
                    # å¤„ç†ç›¸å†Œæ”¹å
                    if latest_data and "albums" in latest_data:
                        old_albums = {a["album_id"]: a.get("name") for a in latest_data["albums"]}
                        if album_id in old_albums and old_albums[album_id] != album_name:
                            old_name = old_albums[album_id]
                            if old_name:
                                old_path = Path(self.plugin_data_dir) / str(group_id) / "albums" / old_name
                                new_path = Path(self.plugin_data_dir) / str(group_id) / "albums" / album_name
                                if old_path.exists() and not new_path.exists():
                                    logger.info(f"æ£€æµ‹åˆ°ç›¸å†Œæ”¹å: {old_name} -> {album_name}ã€‚æ­£åœ¨é‡å‘½åæ–‡ä»¶å¤¹ã€‚")
                                    try:
                                        shutil.move(str(old_path), str(new_path))
                                        self._append_log(group_id, "content_changes", {
                                            "type": "ç›¸å†Œå·²æ”¹å",
                                            "album_id": album_id,
                                            "old_name": old_name,
                                            "new_name": album_name
                                        })
                                    except Exception as e:
                                        logger.error(f"é‡å‘½åç›¸å†Œæ–‡ä»¶å¤¹å¤±è´¥: {e}")
                    
                    # æ£€æŸ¥ç›¸å†Œæ˜¯å¦æœ‰æ›´æ–°ï¼ˆé€šè¿‡ä¿®æ”¹æ—¶é—´ï¼‰
                    media_list = []
                    is_album_updated = True
                    old_media_list = []
                    if latest_data and "albums" in latest_data and "album_media" in latest_data:
                        old_album_info = next((a for a in latest_data["albums"] if a["album_id"] == album_id), None)
                        old_media_list = latest_data["album_media"].get(album_id, [])
                        if old_album_info and str(old_album_info.get("modify_time")) == str(album.get("modify_time")):
                            media_list = old_media_list
                            if media_list:
                                is_album_updated = False
                                logger.debug(f"ç›¸å†Œ {album_name} ä¿®æ”¹æ—¶é—´æœªå˜ï¼Œè·³è¿‡ API è¯·æ±‚ï¼Œå¤ç”¨ä¸Šæ¬¡å¤‡ä»½çš„ {len(media_list)} ä¸ªåª’ä½“è®°å½•ã€‚")
                    
                    if is_album_updated:
                        try:
                            result = await client.get_group_album_media_list(group_id=str(group_id), album_id=album_id)
                            if isinstance(result, dict) and result.get("retcode", 0) != 0:
                                raise Exception(f"API å“åº”å¼‚å¸¸: {result}")
                            logger.debug(f"è·å–ç›¸å†Œ {album_name}({album_id}) åª’ä½“åˆ—è¡¨ç»“æœ: {json.dumps(result, ensure_ascii=False)}")
                            
                            raw_media_list = []
                            if isinstance(result, dict):
                                # æ£€æŸ¥æ—¥å¿—ä¸­çš„ media_list é”®
                                if "media_list" in result:
                                    raw_media_list = result["media_list"]
                                elif "media" in result:
                                    raw_media_list = result["media"]
                                elif "m_media" in result:
                                    raw_media_list = result["m_media"]
                                elif "album" in result:
                                    album_info = result["album"]
                                    if "cover" in album_info and "image" in album_info["cover"]:
                                        raw_media_list = [album_info["cover"]]
                            elif isinstance(result, list):
                                raw_media_list = result
                            
                            for m in raw_media_list:
                                media_type = m.get("type") # 0:å›¾ç‰‡, 1:è§†é¢‘ (åŸºäº log.txt)
                                best_url = ""
                                media_id = ""

                                if media_type == 0: # å›¾ç‰‡
                                    img_detail = m.get("image")
                                    if img_detail:
                                        media_id = img_detail.get("lloc")
                                        photo_urls = img_detail.get("photo_url", [])
                                        # ä¼˜å…ˆé€‰æ‹© spec 1 æˆ– 6
                                        for p in photo_urls:
                                            if p.get("spec") in [1, 6]:
                                                best_url = p.get("url", {}).get("url", "")
                                                break
                                        if not best_url and photo_urls:
                                            best_url = photo_urls[0].get("url", {}).get("url", "")
                                        if not best_url:
                                            best_url = img_detail.get("default_url", {}).get("url", "")

                                elif media_type == 1: # è§†é¢‘
                                    video_detail = m.get("video")
                                    if video_detail:
                                        media_id = video_detail.get("id")
                                        # ä¼˜å…ˆä» video_url åˆ—è¡¨è·å–
                                        video_urls = video_detail.get("video_url", [])
                                        if video_urls:
                                            best_url = video_urls[0].get("url", {}).get("url", "")
                                        # å¤‡é€‰ä½¿ç”¨ç›´æ¥çš„ url å­—ç¬¦ä¸²
                                        if not best_url:
                                            best_url = video_detail.get("url")

                                if best_url:
                                    media_list.append({
                                        "media_id": media_id or m.get("id"),
                                        "url": best_url,
                                        "media_type": media_type,
                                        "upload_time": m.get("upload_time")
                                    })
                                else:
                                    logger.warning(f"æœªèƒ½ä»åª’ä½“é¡¹æå–åˆ°æœ‰æ•ˆ URL: {json.dumps(m, ensure_ascii=False)}")
                        except Exception as e:
                            logger.error(f"è·å–ç›¸å†Œ {album_id} åª’ä½“åˆ—è¡¨å¤±è´¥: {e}")
                            if old_media_list:
                                media_list = old_media_list
                                logger.warning(f"ç”±äº API è¯·æ±‚å¤±è´¥ï¼Œç›¸å†Œ {album_name} æš‚æ—¶å¤ç”¨æ—§å¤‡ä»½æ•°æ®ã€‚")
                    
                    album_media_map[album_id] = media_list
                    
                    # ä¸‹è½½/æ£€æŸ¥æœ¬åœ°æ–‡ä»¶
                    if media_list:
                        album_save_dir = Path(self.plugin_data_dir) / str(group_id) / "albums" / album_name
                        album_save_dir.mkdir(parents=True, exist_ok=True)
                        
                        if is_album_updated:
                            logger.info(f"æ­£åœ¨ä¸‹è½½ç›¸å†Œ {album_name} ä¸­çš„ {len(media_list)} ä¸ªåª’ä½“æ–‡ä»¶...")
                            download_tasks = []
                            for media in media_list:
                                url = media.get("url")
                                media_id = media.get("media_id")
                                if url and media_id:
                                    file_ext = ".jpg" 
                                    if media.get("media_type") == 1:
                                        file_ext = ".mp4"
                                    save_path = album_save_dir / f"{media_id}{file_ext}"
                                    download_tasks.append(self._download_file(url, save_path))
                            if download_tasks:
                                await asyncio.gather(*download_tasks)
                        else:
                            # ä»…æ£€æŸ¥ç¼ºå¤±æ–‡ä»¶å¹¶è­¦å‘Šï¼Œä¸å°è¯•ä½¿ç”¨è¿‡æœŸ URL ä¸‹è½½
                            missing_count = 0
                            for media in media_list:
                                media_id = media.get("media_id")
                                file_ext = ".jpg" if media.get("media_type") == 0 else ".mp4"
                                if not (album_save_dir / f"{media_id}{file_ext}").exists():
                                    missing_count += 1
                            if missing_count > 0:
                                logger.warning(f"ç›¸å†Œ {album_name} æœ‰ {missing_count} ä¸ªæœ¬åœ°æ–‡ä»¶ç¼ºå¤±ï¼Œä½†ç”±äºç›¸å†Œæœªæ›´æ–°ä¸” URL å¯èƒ½å·²è¿‡æœŸï¼Œè·³è¿‡ä¸‹è½½ã€‚è¯·å°è¯•åœ¨ç›¸å†Œæœ‰æ–°ä¸Šä¼ åå†å¤‡ä»½ã€‚")
        except Exception as e:
            logger.error(f"å¤‡ä»½ç¾¤ç›¸å†Œå¤±è´¥: {e}")
        return albums, album_media_map

    @filter.command("ç¾¤å¤‡ä»½")
    async def group_backup(self, event: AstrMessageEvent, group_id_arg: str = ""):
        """ç¾¤å¤‡ä»½ [ç¾¤å·]ï¼šå¤‡ä»½å½“å‰ç¾¤æˆ–æŒ‡å®šç¾¤æ•°æ®åˆ°æœ¬åœ° JSON"""
        # æƒé™æ£€æŸ¥ï¼šBot ç®¡ç†å‘˜ æˆ– é…ç½®é¡¹ä¸­çš„ç®¡ç†å‘˜
        is_admin = event.is_admin()
        user_id = int(event.get_sender_id())
        if not is_admin and (not self.admin_users or user_id not in self.admin_users):
            yield event.plain_result(f"âŒ æ­¤æŒ‡ä»¤ä»…é™ç®¡ç†å‘˜ä½¿ç”¨")
            return

        target_group_id = group_id_arg.strip()
        if not target_group_id:
            target_group_id = event.get_group_id()
        
        if not target_group_id:
            yield event.plain_result("è¯·åœ¨ç¾¤èŠä¸­ä½¿ç”¨æ­¤æŒ‡ä»¤ï¼Œæˆ–åœ¨æŒ‡ä»¤åè·Ÿéšç¾¤å·ã€‚")
            return

        try:
            group_id = int(target_group_id)
            client = event.bot
            
            yield event.plain_result(f"å¼€å§‹å¤‡ä»½ç¾¤ {group_id} çš„æ•°æ®...")

            # åŠ è½½ä¸Šä¸€æ¬¡å¤‡ä»½çš„æ•°æ®ç”¨äºå¢é‡å¯¹æ¯”
            latest_data = self._get_latest_backup_data(group_id)
            
            # 1. è·å–è¯¦ç»†ä¿¡æ¯ (åŒ…å«åŸºç¡€ä¿¡æ¯)
            group_detail = {}
            if "ç¾¤ä¿¡æ¯" in self.backup_options:
                try:
                    raw_detail = await client.get_group_detail_info(group_id=group_id)
                    logger.debug(f"API å“åº” (get_group_detail_info): {json.dumps(raw_detail, ensure_ascii=False)}")
                    
                    # ç²¾ç®€ç¾¤è¯¦ç»†ä¿¡æ¯
                    essential_detail_keys = [
                        "groupCode", "groupName", "ownerUin", "memberNum", "maxMemberNum", 
                        "groupCreateTime", "activeMemberNum", "groupGrade",
                        "group_all_shut", "groupClassText"
                    ]
                    group_detail = {k: raw_detail.get(k) for k in essential_detail_keys if k in raw_detail}
                            
                except Exception as e:
                    logger.warning(f"è·å–ç¾¤ä¿¡æ¯å¤±è´¥: {e}")

            # 1.1 è·å–ç¾¤å¤´åƒ
            if "ç¾¤å¤´åƒ" in self.backup_options:
                try:
                    avatar_url = f"http://p.qlogo.cn/gh/{group_id}/{group_id}/640/"
                    avatar_dir = Path(self.plugin_data_dir) / str(group_id)
                    avatar_save_path = avatar_dir / "group_avatar.png"
                    temp_avatar_path = avatar_dir / "temp_avatar.png"
                    
                    # å…ˆä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
                    await self._download_file(avatar_url, temp_avatar_path, overwrite=True)
                    
                    if temp_avatar_path.exists():
                        import hashlib
                        is_updated = True
                        if avatar_save_path.exists():
                            with open(avatar_save_path, "rb") as f:
                                old_md5 = hashlib.md5(f.read()).hexdigest()
                            with open(temp_avatar_path, "rb") as f:
                                new_md5 = hashlib.md5(f.read()).hexdigest()
                            
                            if old_md5 == new_md5:
                                is_updated = False
                        
                        if is_updated:
                            if avatar_save_path.exists():
                                # å½’æ¡£æ—§å¤´åƒ
                                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                                archive_path = avatar_dir / "logs" / "deleted_items" / f"avatar_{timestamp}.png"
                                archive_path.parent.mkdir(parents=True, exist_ok=True)
                                import shutil
                                shutil.copy2(avatar_save_path, archive_path)
                                self._append_log(group_id, "content_changes", {"type": "ç¾¤å¤´åƒæ›´æ–°", "old_avatar": archive_path.name})
                                logger.info(f"æ£€æµ‹åˆ°ç¾¤å¤´åƒæ›´æ–°ï¼Œæ—§å¤´åƒå·²å½’æ¡£: {archive_path.name}")
                            
                            # åº”ç”¨æ–°å¤´åƒ
                            if avatar_save_path.exists(): avatar_save_path.unlink()
                            temp_avatar_path.rename(avatar_save_path)
                        else:
                            # æ— å˜åŒ–ï¼Œåˆ é™¤ä¸´æ—¶æ–‡ä»¶
                            temp_avatar_path.unlink()
                except Exception as e:
                    logger.warning(f"è·å–ç¾¤å¤´åƒå¤±è´¥: {e}")

            # 2. è·å–æˆå‘˜åˆ—è¡¨å¹¶ç²¾ç®€
            members = []
            if "ç¾¤æˆå‘˜" in self.backup_options:
                raw_members = await client.get_group_member_list(group_id=group_id)
                logger.debug(f"API å“åº” (get_group_member_list): è·å–åˆ° {len(raw_members)} åæˆå‘˜ã€‚")
                essential_keys = ["user_id", "nickname", "card", "role", "level", "title", "join_time", "last_sent_time"]
                for m in raw_members:
                    members.append({k: m.get(k) for k in essential_keys if k in m})
                
                # å¢é‡å¯¹æ¯”ç¾¤æˆå‘˜
                if latest_data and "members" in latest_data:
                    old_members_map = {m["user_id"]: m for m in latest_data["members"]}
                    new_members_map = {m["user_id"]: m for m in members}
                    
                    # è°è¿›ç¾¤äº†
                    joiners = [m for uid, m in new_members_map.items() if uid not in old_members_map]
                    # è°é€€ç¾¤äº†
                    leavers = [m for uid, m in old_members_map.items() if uid not in new_members_map]
                    
                    if joiners:
                        logger.info(f"æ£€æµ‹åˆ°æ–°æˆå‘˜è¿›ç¾¤: {joiners}")
                        for m in joiners:
                            self._append_log(group_id, "member_changes", {"type": "å…¥ç¾¤", "user_id": m["user_id"], "nickname": m.get("nickname")})
                    if leavers:
                        logger.info(f"æ£€æµ‹åˆ°æˆå‘˜é€€ç¾¤: {leavers}")
                        for m in leavers:
                            self._append_log(group_id, "member_changes", {"type": "é€€ç¾¤", "user_id": m["user_id"], "nickname": m.get("nickname")})
            
            # 3. è·å–å…¬å‘Š
            notices = []
            if "ç¾¤å…¬å‘Š" in self.backup_options:
                try:
                    raw_notices = await client._get_group_notice(group_id=group_id)
                    logger.debug(f"API å“åº” (_get_group_notice): {json.dumps(raw_notices, ensure_ascii=False)}")
                    
                    # ç²¾ç®€å…¬å‘Šä¿¡æ¯
                    for n in raw_notices:
                        msg = n.get("message", {})
                        notice_item = {
                            "notice_id": n.get("notice_id"),
                            "sender_id": n.get("sender_id"),
                            "publish_time": n.get("publish_time"),
                            "text": msg.get("text", "")
                        }
                        # è§£æå›¾ç‰‡ä¿¡æ¯
                        images = msg.get("image") or msg.get("images")
                        if images:
                            if not isinstance(images, list):
                                images = [images]
                            
                            processed_images = []
                            notice_img_dir = Path(self.plugin_data_dir) / str(group_id) / "notices_images"
                            for img in images:
                                if isinstance(img, dict):
                                    img_id = img.get("id")
                                    size = "628"
                                    img_url = f"https://gdynamic.qpic.cn/gdynamic/{img_id}/{size}"
                                    
                                    img["url"] = img_url
                                    # å¤‡ä»½å›¾ç‰‡åˆ°æœ¬åœ°
                                    ext = ".jpg" 
                                    local_path = notice_img_dir / f"{img_id}{ext}"
                                    success = await self._download_file(img_url, local_path)
                                    if success:
                                        img["local_path"] = str(local_path.relative_to(Path(self.plugin_data_dir) / str(group_id)))
                                            
                                    processed_images.append(img)
                                else:
                                    processed_images.append(img)
                            notice_item["images"] = processed_images
                        notices.append(notice_item)
                    
                    # å¢é‡å¯¹æ¯”ç¾¤å…¬å‘Š
                    if latest_data and "notices" in latest_data:
                        old_notices_map = {n["notice_id"]: n for n in latest_data["notices"]}
                        new_notices_map = {n["notice_id"]: n for n in notices}
                        
                        # æ£€æµ‹æ–°å¢
                        joiners = [n for nid, n in new_notices_map.items() if nid not in old_notices_map]
                        if joiners:
                            for n in joiners:
                                self._append_log(group_id, "content_changes", {"type": "æ–°å¢å…¬å‘Š", "notice_id": n["notice_id"], "text": n["text"]})
                        
                        # æ£€æµ‹åˆ é™¤
                        deleted_notices = [n for nid, n in old_notices_map.items() if nid not in new_notices_map]
                        if deleted_notices:
                            logger.info(f"æ£€æµ‹åˆ°å·²åˆ é™¤çš„ç¾¤å…¬å‘Š: {deleted_notices}")
                            self._archive_deleted_items(group_id, "notices", deleted_notices)
                            for n in deleted_notices:
                                self._append_log(group_id, "content_changes", {"type": "å…¬å‘Šå·²åˆ é™¤", "notice_id": n["notice_id"]})
                except Exception as e:
                    logger.warning(f"è·å–ç¾¤å…¬å‘Šå¤±è´¥: {e}")
                
            # 4. è·å–ç¾¤ç²¾å
            essence = []
            if "ç¾¤ç²¾å" in self.backup_options:
                try:
                    raw_essence = await client.get_essence_msg_list(group_id=group_id)
                    logger.debug(f"API å“åº” (get_essence_msg_list): {json.dumps(raw_essence, ensure_ascii=False)}")
                    
                    # ç¡®ä¿ raw_essence æ˜¯åˆ—è¡¨
                    if isinstance(raw_essence, dict) and "data" in raw_essence:
                        raw_essence = raw_essence["data"]
                    
                    if raw_essence and isinstance(raw_essence, list):
                        # ç²¾ç®€ç¾¤ç²¾åå¹¶è·å–å‘é€æ—¶é—´
                        for e in raw_essence:
                            # å¤„ç†ç²¾åæ¶ˆæ¯ä¸­çš„å›¾ç‰‡
                            essence_img_dir = Path(self.plugin_data_dir) / str(group_id) / "essence_images"
                            content = e.get("content")
                            if content:
                                if not isinstance(content, list):
                                    content = [content]
                                
                                for seg in content:
                                    if isinstance(seg, dict) and seg.get("type") == "image":
                                        data = seg.get("data", {})
                                        img_id = data.get("file_id") or data.get("file")
                                        img_url = data.get("url")
                                        
                                        # å¦‚æœæ²¡æœ‰ URL ä½†æœ‰ IDï¼Œæ„é€ æŠ“åŒ…æ ¼å¼çš„ URL
                                        if not img_url and img_id:
                                            img_url = f"https://gdynamic.qpic.cn/gdynamic/{img_id}/628"
                                            data["url"] = img_url
                                        
                                        if img_url:
                                            # å¤‡ä»½å›¾ç‰‡åˆ°æœ¬åœ°
                                            ext = ".jpg"
                                            file_name = img_id if img_id else hashlib.md5(img_url.encode()).hexdigest()
                                            local_path = essence_img_dir / f"{file_name}{ext}"
                                            success = await self._download_file(img_url, local_path)
                                            if success:
                                                data["local_path"] = str(local_path.relative_to(Path(self.plugin_data_dir) / str(group_id)))

                            essence.append({
                                "message_id": e.get("message_id"),
                                "sender_id": e.get("sender_id"),
                                "sender_nick": e.get("sender_nick"),
                                "operator_id": e.get("operator_id"),
                                "operator_nick": e.get("operator_nick"),
                                "operator_time": e.get("operator_time"),
                                "content": e.get("content")
                            })
                    
                    # å¢é‡å¯¹æ¯”ç¾¤ç²¾å
                    if latest_data and "essence" in latest_data:
                        old_essence_map = {e["message_id"]: e for e in latest_data["essence"]}
                        new_essence_map = {e["message_id"]: e for e in essence}
                        
                        deleted_essence = [e for mid, e in old_essence_map.items() if mid not in new_essence_map]
                        if deleted_essence:
                            logger.info(f"æ£€æµ‹åˆ°å·²åˆ é™¤çš„ç¾¤ç²¾å: {deleted_essence}")
                            self._archive_deleted_items(group_id, "essence", deleted_essence)
                            for e in deleted_essence:
                                self._append_log(group_id, "content_changes", {"type": "ç¾¤ç²¾åå·²åˆ é™¤", "message_id": e["message_id"]})
                except Exception as e:
                    logger.warning(f"è·å–ç¾¤ç²¾åå¤±è´¥: {e}")
                
            # 5. è·å–ç¾¤è£èª‰
            honors = {}
            if "ç¾¤è£èª‰" in self.backup_options:
                try:
                    honors = await client.get_group_honor_info(group_id=group_id, type="all")
                    logger.debug(f"API å“åº” (get_group_honor_info): {json.dumps(honors, ensure_ascii=False)}")
                except Exception as e:
                    logger.warning(f"è·å–ç¾¤è£èª‰å¤±è´¥: {e}")

            # 6. è·å–ç¾¤ç›¸å†Œå¹¶å¤‡ä»½åŸå›¾
            albums = []
            album_media_map = {}
            if "ç¾¤ç›¸å†Œ" in self.backup_options:
                albums, album_media_map = await self._backup_albums(client, group_id, latest_data)

            # 7. å¢é‡å¯¹æ¯”ç¾¤ç›¸å†Œï¼ˆå¤„ç†å·²åˆ é™¤çš„å›¾ç‰‡/ç›¸å†Œï¼‰
            if "ç¾¤ç›¸å†Œ" in self.backup_options and latest_data and "album_media" in latest_data:
                try:
                    old_album_media = latest_data["album_media"]
                    # æŸ¥æ‰¾å·²åˆ é™¤çš„ç›¸å†Œ
                    for old_album_id, old_media_list in old_album_media.items():
                        if old_album_id not in album_media_map:
                            # æ•´ä¸ªç›¸å†Œè¢«åˆ äº†
                            self._append_log(group_id, "content_changes", {"type": "ç›¸å†Œå·²åˆ é™¤", "album_id": old_album_id})
                            old_albums_list = latest_data.get("albums", [])
                            old_album_info = next((a for a in old_albums_list if a["album_id"] == old_album_id), {"album_id": old_album_id, "name": "æœªçŸ¥ç›¸å†Œ"})
                            self._archive_deleted_items(group_id, "albums", [old_album_info])
                            
                            old_name = old_album_info.get("name")
                            if old_name:
                                src_dir = Path(self.plugin_data_dir) / str(group_id) / "albums" / old_name
                                if src_dir.exists():
                                    dst_dir = Path(self.plugin_data_dir) / str(group_id) / "logs" / "deleted_items" / "albums" / old_name
                                    dst_dir.parent.mkdir(parents=True, exist_ok=True)
                                    import shutil
                                    if dst_dir.exists(): shutil.rmtree(dst_dir)
                                    logger.info(f"æ­£åœ¨å°†å·²åˆ é™¤çš„ç›¸å†Œç›®å½•ä» {src_dir} ç§»åŠ¨åˆ° {dst_dir}")
                                    shutil.move(str(src_dir), str(dst_dir))
                        else:
                            # ç›¸å†Œè¿˜åœ¨ï¼Œæ£€æŸ¥é‡Œé¢çš„å›¾ç‰‡æœ‰æ²¡æœ‰è¢«åˆ 
                            new_media_ids = {m["media_id"] for m in album_media_map[old_album_id]}
                            deleted_media = [m for m in old_media_list if m["media_id"] not in new_media_ids]
                            
                            if deleted_media:
                                # è®°å½•æ—¥å¿—
                                self._archive_deleted_items(group_id, "media", deleted_media)
                                for m in deleted_media:
                                    self._append_log(group_id, "content_changes", {
                                        "type": "åª’ä½“æ–‡ä»¶å·²åˆ é™¤", 
                                        "album_id": old_album_id, 
                                        "media_id": m["media_id"]
                                    })
                                
                                # å°†è¢«åˆ å›¾ç‰‡ç§»åŠ¨åˆ°å›æ”¶ç«™
                                # æ‰¾åˆ°å½“å‰ç›¸å†Œçš„æ–‡ä»¶å¤¹å
                                current_albums = {a["album_id"]: a["name"] for a in albums}
                                album_name = current_albums.get(old_album_id)
                                if album_name:
                                    for m in deleted_media:
                                        # å°è¯•ä¸åŒçš„å¯èƒ½åç¼€
                                        for ext in [".jpg", ".mp4", ".png"]:
                                            src_file = Path(self.plugin_data_dir) / str(group_id) / "albums" / album_name / f"{m['media_id']}{ext}"
                                            if src_file.exists():
                                                dst_file = Path(self.plugin_data_dir) / str(group_id) / "logs" / "deleted_items" / "albums" / album_name / f"{m['media_id']}{ext}"
                                                dst_file.parent.mkdir(parents=True, exist_ok=True)
                                                logger.info(f"æ­£åœ¨å°†å·²åˆ é™¤çš„åª’ä½“æ–‡ä»¶ä» {src_file} ç§»åŠ¨åˆ° {dst_file}")
                                                import shutil
                                                shutil.move(str(src_file), str(dst_file))
                                                break
                except Exception as e:
                    logger.error(f"å¤„ç†å·²åˆ é™¤ç›¸å†Œå›¾ç‰‡æ—¶å‡ºé”™: {e}")

            # å‡†å¤‡æ•°æ®ç›®å½•
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            group_base_dir = Path(self.plugin_data_dir) / str(group_id)
            backup_path = group_base_dir / timestamp
            backup_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ JSON
            data_to_save = {
                "group_detail": group_detail,
                "members": members,
                "notices": notices,
                "essence": essence,
                "honors": honors,
                "albums": albums,
                "album_media": album_media_map
            }
            
            # æ‰§è¡Œä¿å­˜
            for key, val in data_to_save.items():
                file_name = f"{key}.json"
                save_file_path = backup_path / file_name
                with open(save_file_path, "w", encoding="utf-8") as f:
                    json.dump(val, f, ensure_ascii=False, indent=4)
                logger.info(f"æˆåŠŸä¿å­˜å¤‡ä»½å¿«ç…§æ–‡ä»¶: {save_file_path}")
            
            # åˆ é™¤é™¤å½“å‰åˆšåˆ›å»ºçš„å¤‡ä»½ä»¥å¤–çš„æ‰€æœ‰æ—§å¿«ç…§æ–‡ä»¶å¤¹
            all_backups = sorted([d for d in group_base_dir.iterdir() if d.is_dir() and d.name.replace("_", "").isdigit()], key=lambda x: x.name)
            for old_backup in all_backups:
                if old_backup.name != timestamp:
                    try:
                        import shutil
                        shutil.rmtree(str(old_backup))
                        logger.info(f"å·²æ¸…ç†æ—§å¤‡ä»½å¿«ç…§: {old_backup.name}")
                    except Exception as e:
                        logger.error(f"æ¸…ç†æ—§å¤‡ä»½å¤±è´¥ {old_backup.name}: {e}")
            
            yield event.plain_result(f"âœ… ç¾¤ {group_id} å¤‡ä»½æˆåŠŸï¼")
            
        except Exception as e:
            logger.error(f"ç¾¤å¤‡ä»½å‡ºé”™: {e}")
            yield event.plain_result(f"âŒ å¤‡ä»½å¤±è´¥: {e}")

    @filter.command("åˆ é™¤ç¾¤å¤‡ä»½")
    async def delete_group_backup(self, event: AstrMessageEvent, group_id_arg: str = ""):
        """åˆ é™¤ç¾¤å¤‡ä»½ [ç¾¤å·]ï¼šç‰©ç†åˆ é™¤æŒ‡å®šç¾¤ç»„çš„æ‰€æœ‰å¤‡ä»½æ•°æ®"""
        # æƒé™æ£€æŸ¥
        is_admin = event.is_admin()
        user_id = int(event.get_sender_id())
        if not is_admin and (not self.admin_users or user_id not in self.admin_users):
            yield event.plain_result(f"âŒ æ­¤æŒ‡ä»¤ä»…é™ç®¡ç†å‘˜ä½¿ç”¨")
            return

        target_group_id = group_id_arg.strip()
        if not target_group_id:
            target_group_id = event.get_group_id()
        
        if not target_group_id:
            yield event.plain_result("è¯·åœ¨ç¾¤èŠä¸­ä½¿ç”¨æ­¤æŒ‡ä»¤ï¼Œæˆ–åœ¨æŒ‡ä»¤åè·Ÿéšç¾¤å·ã€‚")
            return

        try:
            group_id = int(target_group_id)
            group_dir = Path(self.plugin_data_dir) / str(group_id)
            
            if not group_dir.exists():
                yield event.plain_result(f"ğŸ” æœªæ‰¾åˆ°ç¾¤ {group_id} çš„å¤‡ä»½æ•°æ®ã€‚")
                return

            import shutil
            # ç‰©ç†åˆ é™¤æ•´ä¸ªç¾¤ç›®å½•
            shutil.rmtree(str(group_dir))
            
            logger.info(f"ç®¡ç†å‘˜ {user_id} åˆ é™¤äº†ç¾¤ {group_id} çš„æ‰€æœ‰å¤‡ä»½æ•°æ®ã€‚")
            yield event.plain_result(f"âœ… å·²æˆåŠŸåˆ é™¤ç¾¤ {group_id} çš„æ‰€æœ‰å¤‡ä»½æ•°æ®ï¼ˆåŒ…æ‹¬ç›¸å†Œå’Œæ—¥å¿—ï¼‰ã€‚")

        except Exception as e:
            logger.error(f"åˆ é™¤ç¾¤å¤‡ä»½å‡ºé”™: {e}")
            yield event.plain_result(f"âŒ åˆ é™¤å¤±è´¥: {e}")
    @filter.command("ç¾¤å¯¼å‡º")
    async def group_export(self, event: AstrMessageEvent, args: str = ""):
        """ç¾¤å¯¼å‡º [ç¾¤å·] [é€‰é¡¹...]ï¼šå¯¼å‡ºæŒ‡å®šæ•°æ®ã€‚é€‰é¡¹å¯é€‰ï¼šç¾¤ä¿¡æ¯ã€ç¾¤æˆå‘˜ã€ç¾¤å…¬å‘Šã€ç¾¤ç²¾åã€ç¾¤è£èª‰ã€ç¾¤ç›¸å†Œ"""
        # æƒé™æ£€æŸ¥
        is_admin = event.is_admin()
        user_id = int(event.get_sender_id())
        if not is_admin and (not self.admin_users or user_id not in self.admin_users):
            yield event.plain_result(f"âŒ æ­¤æŒ‡ä»¤ä»…é™ç®¡ç†å‘˜ä½¿ç”¨")
            return

        # å‚æ•°è§£æ
        parts = event.message_str.split()
        arg_list = parts[1:]
        
        target_group_id = ""
        requested_options = []
        all_possible_options = ["ç¾¤ä¿¡æ¯", "ç¾¤æˆå‘˜", "ç¾¤å…¬å‘Š", "ç¾¤ç²¾å", "ç¾¤è£èª‰", "ç¾¤ç›¸å†Œ"]
        
        for part in arg_list:
            if part in all_possible_options:
                requested_options.append(part)
            elif part.isdigit():
                target_group_id = part
        
        if not target_group_id:
            target_group_id = event.get_group_id()
        
        if not target_group_id:
            yield event.plain_result("è¯·åœ¨ç¾¤èŠä¸­ä½¿ç”¨æ­¤æŒ‡ä»¤ï¼Œæˆ–åœ¨æŒ‡ä»¤åè·Ÿéšç¾¤å·ã€‚")
            return

        group_id = int(target_group_id)
        # å¦‚æœç”¨æˆ·æ²¡å¡«é€‰é¡¹ï¼Œåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤é€‰é¡¹ï¼Œä½†æ’é™¤ç¾¤ç›¸å†Œï¼ˆç¾¤ç›¸å†Œéœ€æ˜¾å¼æŒ‡å®šï¼‰
        if not requested_options:
            requested_options = [opt for opt in all_possible_options if opt in self.backup_options and opt != "ç¾¤ç›¸å†Œ"]
            # å¦‚æœé…ç½®é‡Œæ²¡å¼€ä»»ä½•é¡¹ï¼ˆæˆ–åªå¼€äº†ç›¸å†Œï¼‰ï¼Œåˆ™é»˜è®¤å¯¼å‡ºé™¤ç›¸å†Œå¤–çš„æ‰€æœ‰
            if not requested_options:
                requested_options = [opt for opt in all_possible_options if opt != "ç¾¤ç›¸å†Œ"]

        try:
            client = event.bot
            logger.info(f"æ”¶åˆ°å¯¼å‡ºè¯·æ±‚: ç¾¤å·={group_id}, é€‰é¡¹={requested_options}, åŸå§‹æ¶ˆæ¯='{event.message_str}'")
            
            # åŠ è½½ä¸Šä¸€æ¬¡å¤‡ä»½çš„æ•°æ®ç”¨äºå¼‚å¸¸å›é€€
            latest_data = self._get_latest_backup_data(group_id)
            
            yield event.plain_result(f"æ­£åœ¨å¯¼å‡ºç¾¤ {group_id} çš„æ•°æ®: {', '.join(requested_options)}...")

            # --- å¤„ç†ç¾¤ç›¸å†Œå¤‡ä»½ä¸æ‰“åŒ… ---
            zip_base64 = None
            if "ç¾¤ç›¸å†Œ" in requested_options:
                # 1. å…ˆæ‰§è¡Œä¸€æ¬¡å¤‡ä»½
                logger.info(f"æ­£åœ¨æ‰§è¡Œç¾¤ {group_id} çš„ç›¸å†Œå¯¼å‡ºå‰å¤‡ä»½...")
                await self._backup_albums(client, group_id, latest_data)
                
                # 2. å‹ç¼©æ‰“åŒ…
                album_dir = Path(self.plugin_data_dir) / str(group_id) / "albums"
                deleted_dir = Path(self.plugin_data_dir) / str(group_id) / "logs" / "deleted_items"
                
                # æ£€æŸ¥ç›®å½•æ˜¯å¦çœŸçš„åŒ…å«æ–‡ä»¶
                def has_files(directory: Path):
                    if not directory.exists(): return False
                    for _, _, files in os.walk(directory):
                        if files: return True
                    return False

                if has_files(album_dir) or has_files(deleted_dir):
                    logger.info(f"æ­£åœ¨å‹ç¼©ç¾¤ {group_id} çš„å¤‡ä»½ç›®å½•ï¼ˆåŒ…å«ç›¸å†Œå’Œå·²åˆ é™¤é¡¹ç›®ï¼‰...")
                    zip_buffer = BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:
                        # æ‰“åŒ…ç°æœ‰ç›¸å†Œ
                        if has_files(album_dir):
                            for root, dirs, files in os.walk(album_dir):
                                for file in files:
                                    file_path = Path(root) / file
                                    arcname = file_path.relative_to(album_dir.parent)
                                    zf.write(file_path, arcname)
                        
                        # æ‰“åŒ…å·²åˆ é™¤é¡¹ç›®ï¼ˆå›æ”¶ç«™ï¼‰
                        if has_files(deleted_dir):
                            for root, dirs, files in os.walk(deleted_dir):
                                for file in files:
                                    file_path = Path(root) / file
                                    # åœ¨å‹ç¼©åŒ…å†…å­˜æ”¾åœ¨ "å›æ”¶ç«™" ç›®å½•ä¸‹
                                    arcname = Path("å›æ”¶ç«™") / file_path.relative_to(deleted_dir)
                                    zf.write(file_path, arcname)
                    
                    zip_content = zip_buffer.getvalue()
                    if zip_content:
                        zip_base64 = base64.b64encode(zip_content).decode("utf-8")
                else:
                    logger.warning(f"ç¾¤ {group_id} çš„ç›¸å†Œç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡å‹ç¼©ã€‚")

            # --- å¤„ç† Excel å¯¼å‡º ---
            excel_base64 = None
            # åªæœ‰å½“è¯·æ±‚äº†é™¤ç¾¤ç›¸å†Œä»¥å¤–çš„é€‰é¡¹æ—¶ï¼Œæ‰ç”Ÿæˆ Excel
            excel_options = [opt for opt in requested_options if opt != "ç¾¤ç›¸å†Œ"]
            
            # å¦‚æœåªè¯·æ±‚äº†ç¾¤ç›¸å†Œï¼Œåˆ™ä¸ç”Ÿæˆ Excel
            if excel_options:
                output_buffer = BytesIO()
                with pd.ExcelWriter(output_buffer, engine="openpyxl") as writer:
                    # 1. å¯¼å‡ºç¾¤æ¦‚å†µ (ç¾¤ä¿¡æ¯)
                    if "ç¾¤ä¿¡æ¯" in requested_options:
                        detail = {}
                        try:
                            raw_res = await client.get_group_detail_info(group_id=group_id)
                            if isinstance(raw_res, dict) and raw_res.get("retcode", 0) != 0:
                                raise Exception(f"API å“åº”å¼‚å¸¸: {raw_res}")
                            detail = raw_res
                        except Exception as e:
                            logger.warning(f"è·å–å®æ—¶ç¾¤æ¦‚å†µå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ä»½æ•°æ®: {e}")
                            detail = latest_data.get("group_detail", {})
                            
                        if detail:
                            display_detail = {
                                "ç¾¤åç§°": detail.get("groupName"),
                                "ç¾¤å·": detail.get("groupCode"),
                                "ç¾¤åˆ†ç±»": detail.get("groupClassText"),
                                "ç¾¤ä¸»QQ": detail.get("ownerUin"),
                                "æˆå‘˜äººæ•°": detail.get("memberNum"),
                                "æœ€å¤§äººæ•°": detail.get("maxMemberNum"),
                                "å½“å‰æ´»è·ƒäººæ•°": detail.get("activeMemberNum"),
                            }
                            detail_list = [{"å±æ€§": k, "å€¼": v} for k, v in display_detail.items() if v is not None]
                            pd.DataFrame(detail_list).to_excel(writer, index=False, sheet_name="ç¾¤æ¦‚å†µ")

                    # 2. å¯¼å‡ºç¾¤æˆå‘˜
                    if "ç¾¤æˆå‘˜" in requested_options:
                        members = []
                        try:
                            raw_res = await client.get_group_member_list(group_id=group_id)
                            if isinstance(raw_res, dict) and raw_res.get("retcode", 0) != 0:
                                raise Exception(f"API å“åº”å¼‚å¸¸: {raw_res}")
                            members = raw_res
                        except Exception as e:
                            logger.warning(f"è·å–å®æ—¶ç¾¤æˆå‘˜å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ä»½æ•°æ®: {e}")
                            members = latest_data.get("members", [])

                        if members:
                            processed_members = []
                            for m in members:
                                item = {}
                                for opt, api_key in self.field_map.items():
                                    val = m.get(api_key, "")
                                    if api_key in ["join_time", "last_sent_time"]:
                                        val = self._format_timestamp(val)
                                    elif api_key == "role":
                                        val = {"owner": "ç¾¤ä¸»", "admin": "ç®¡ç†å‘˜", "member": "æˆå‘˜"}.get(val, val)
                                    item[opt] = val
                                processed_members.append(item)
                            pd.DataFrame(processed_members).to_excel(writer, index=False, sheet_name="ç¾¤æˆå‘˜")

                    # 3. å¯¼å‡ºç¾¤å…¬å‘Š
                    if "ç¾¤å…¬å‘Š" in requested_options:
                        notices = []
                        try:
                            raw_res = await client._get_group_notice(group_id=group_id)
                            if isinstance(raw_res, dict) and raw_res.get("retcode", 0) != 0:
                                raise Exception(f"API å“åº”å¼‚å¸¸: {raw_res}")
                            notices = raw_res
                        except Exception as e:
                            logger.warning(f"è·å–å®æ—¶ç¾¤å…¬å‘Šå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ä»½æ•°æ®: {e}")
                            notices = latest_data.get("notices", [])

                        if notices:
                            processed_notices = []
                            for n in notices:
                                msg = n.get("message", {})
                                content = msg.get("text", "")
                                
                                # æ ¼å¼åŒ–å¤„ç†ï¼šæ›¿æ¢ HTML å®ä½“
                                if content:
                                    content = content.replace("&#10;", "\n").replace("&nbsp;", " ")
                                
                                # æ£€æŸ¥å¹¶æ·»åŠ å›¾ç‰‡ URL
                                images = msg.get("image") or msg.get("images")
                                if images:
                                    if not isinstance(images, list):
                                        images = [images]
                                    urls = []
                                    for img in images:
                                        if isinstance(img, dict):
                                            img_id = img.get("id")
                                            url = img.get("url")
                                            if not url and img_id:
                                                # ç»Ÿä¸€ä½¿ç”¨æŠ“åŒ…æ ¼å¼çš„ URL
                                                url = f"https://gdynamic.qpic.cn/gdynamic/{img_id}/628"
                                            if url:
                                                urls.append(url)
                                    if urls:
                                        content += "\nå›¾ç‰‡: " + " | ".join(urls)
                                processed_notices.append({
                                    "å‘å¸ƒè€…": n.get("sender_id"),
                                    "å‘å¸ƒæ—¶é—´": self._format_timestamp(n.get("publish_time")),
                                    "å†…å®¹": content
                                })
                            if processed_notices:
                                pd.DataFrame(processed_notices).to_excel(writer, index=False, sheet_name="ç¾¤å…¬å‘Š")

                    # 4. å¯¼å‡ºç¾¤ç²¾å
                    if "ç¾¤ç²¾å" in requested_options:
                        essence = []
                        try:
                            raw_res = await client.get_essence_msg_list(group_id=group_id)
                            if isinstance(raw_res, dict) and raw_res.get("retcode", 0) != 0:
                                raise Exception(f"API å“åº”å¼‚å¸¸: {raw_res}")
                            essence = raw_res
                        except Exception as e:
                            logger.warning(f"è·å–å®æ—¶ç¾¤ç²¾åå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ä»½æ•°æ®: {e}")
                            essence = latest_data.get("essence", [])

                        if isinstance(essence, dict) and "data" in essence:
                            essence = essence["data"]
                        if essence and isinstance(essence, list):
                            processed_essence = []
                            for e in essence:
                                processed_essence.append({
                                    "å‘é€è€…": e.get("sender_id"),
                                    "è®¾ç²¾æ—¶é—´": self._format_timestamp(e.get("operator_time")),
                                    "å†…å®¹": self._format_essence_content(e.get("content", [])),
                                    "æ“ä½œè€…": e.get("operator_id")
                                })
                            pd.DataFrame(processed_essence).to_excel(writer, index=False, sheet_name="ç¾¤ç²¾å")

                    # 5. å¯¼å‡ºç¾¤è£èª‰
                    if "ç¾¤è£èª‰" in requested_options:
                        honors = {}
                        try:
                            raw_res = await client.get_group_honor_info(group_id=group_id, type="all")
                            if isinstance(raw_res, dict) and raw_res.get("retcode", 0) != 0:
                                raise Exception(f"API å“åº”å¼‚å¸¸: {raw_res}")
                            honors = raw_res
                        except Exception as e:
                            logger.warning(f"è·å–å®æ—¶ç¾¤è£èª‰å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ä»½æ•°æ®: {e}")
                            honors = latest_data.get("honors", {})

                        if honors:
                            honor_list = []
                            honor_type_map = {
                                "current_talkative": "é¾™ç‹",
                                "talkative_list": "é¾™ç‹å†å²è·å¾—è€…",
                                "performer_list": "ç¾¤èŠä¹‹ç«",
                                "legend_list": "ç¾¤èŠç‚½ç„°",
                                "emotion_list": "å¿«ä¹æºæ³‰",
                                "strong_newbie_list": "å–„è´¢ç¦ç¦„å¯¿"
                            }
                            for honor_type, honor_data in honors.items():
                                if honor_type == "group_id": continue
                                type_name = honor_type_map.get(honor_type, honor_type)
                                
                                if isinstance(honor_data, dict) and "user_id" in honor_data:
                                    honor_list.append({
                                        "è£èª‰ç±»å‹": type_name, 
                                        "QQå·": honor_data.get("user_id"), 
                                        "æ˜µç§°": honor_data.get("nickname"),
                                        "æè¿°": honor_data.get("description", "")
                                    })
                                elif isinstance(honor_data, list):
                                    for h in honor_data:
                                        honor_list.append({
                                            "è£èª‰ç±»å‹": type_name, 
                                            "QQå·": h.get("user_id"), 
                                            "æ˜µç§°": h.get("nickname"),
                                            "æè¿°": h.get("description", "")
                                        })
                            if honor_list:
                                pd.DataFrame(honor_list).to_excel(writer, index=False, sheet_name="ç¾¤è£èª‰")

                    # 6. å¯¼å‡ºç¾¤ç›¸å†Œåˆ—è¡¨
                    if "ç¾¤ç›¸å†Œ" in requested_options:
                        albums_list = []
                        try:
                            raw_res = await client.get_qun_album_list(group_id=str(group_id))
                            if isinstance(raw_res, dict) and raw_res.get("retcode", 0) != 0:
                                raise Exception(f"API å“åº”å¼‚å¸¸: {raw_res}")
                            albums_list = raw_res
                        except Exception as e:
                            logger.warning(f"è·å–å®æ—¶ç¾¤ç›¸å†Œåˆ—è¡¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ä»½æ•°æ®: {e}")
                            albums_list = latest_data.get("albums", [])

                        if albums_list:
                            processed_albums = []
                            for a in albums_list:
                                processed_albums.append({
                                    "ç›¸å†Œå": a.get("name"),
                                    "å›¾ç‰‡æ•°é‡": a.get("upload_number"),
                                    "åˆ›å»ºè€…": a.get("creator", {}).get("nick"),
                                    "åˆ›å»ºæ—¶é—´": self._format_timestamp(a.get("create_time")),
                                    "ä¿®æ”¹æ—¶é—´": self._format_timestamp(a.get("modify_time"))
                                })
                            pd.DataFrame(processed_albums).to_excel(writer, index=False, sheet_name="ç¾¤ç›¸å†Œåˆ—è¡¨")
                
                    # 7. å¯¼å‡ºå·²åˆ é™¤çš„é¡¹ç›®ï¼ˆå›æ”¶ç«™æ•°æ®ï¼‰
                    archive_file = Path(self.plugin_data_dir) / str(group_id) / "logs" / "deleted_items.json"
                    if archive_file.exists():
                        try:
                            with open(archive_file, "r", encoding="utf-8") as f:
                                archive = json.load(f)
                            
                            for item_type, items in archive.items():
                                if not items: continue
                                
                                # åªæœ‰å½“ç”¨æˆ·è¯·æ±‚äº†å¯¹åº”çš„ä¸»è¦é€‰é¡¹æ—¶ï¼Œæ‰å¯¼å‡ºå¯¹åº”çš„å·²åˆ é™¤é¡¹ç›®
                                if item_type == "notices" and "ç¾¤å…¬å‘Š" not in requested_options: continue
                                if item_type == "essence" and "ç¾¤ç²¾å" not in requested_options: continue
                                if item_type in ["albums", "media"] and "ç¾¤ç›¸å†Œ" not in requested_options: continue
                                
                                sheet_name_map = {
                                    "notices": "å·²åˆ é™¤å…¬å‘Š",
                                    "essence": "å·²åˆ é™¤ç²¾å",
                                    "albums": "å·²åˆ é™¤ç›¸å†Œ",
                                    "media": "å·²åˆ é™¤åª’ä½“"
                                }
                                sheet_name = sheet_name_map.get(item_type, f"å·²åˆ é™¤_{item_type}")
                                
                                processed_items = []
                                for item in items:
                                    deleted_at = item.get("deleted_at", "æœªçŸ¥")
                                    content = item.get("content", {})
                                    
                                    if item_type == "notices":
                                        processed_items.append({
                                            "åˆ é™¤æ—¶é—´": deleted_at,
                                            "å‘å¸ƒè€…": content.get("sender_id"),
                                            "å‘å¸ƒæ—¶é—´": self._format_timestamp(content.get("publish_time")),
                                            "å†…å®¹": content.get("text")
                                        })
                                    elif item_type == "essence":
                                        processed_items.append({
                                            "åˆ é™¤æ—¶é—´": deleted_at,
                                            "å‘é€è€…": content.get("sender_id"),
                                            "è®¾ç²¾æ—¶é—´": self._format_timestamp(content.get("operator_time")),
                                            "å†…å®¹": self._format_essence_content(content.get("content", []))
                                        })
                                    elif item_type == "albums":
                                        processed_items.append({
                                            "åˆ é™¤æ—¶é—´": deleted_at,
                                            "ç›¸å†ŒID": content.get("album_id"),
                                            "ç›¸å†Œå": content.get("name")
                                        })
                                    elif item_type == "media":
                                        processed_items.append({
                                            "åˆ é™¤æ—¶é—´": deleted_at,
                                            "åª’ä½“ID": content.get("media_id"),
                                            "ç±»å‹": "å›¾ç‰‡" if content.get("media_type") == 0 else "è§†é¢‘",
                                            "åŸå§‹URL": content.get("url")
                                        })
                                    else:
                                        # é€šç”¨å¤„ç†
                                        processed_items.append({
                                            "åˆ é™¤æ—¶é—´": deleted_at,
                                            "åŸå§‹å†…å®¹": json.dumps(content, ensure_ascii=False)
                                        })
                                
                                if processed_items:
                                    pd.DataFrame(processed_items).to_excel(writer, index=False, sheet_name=sheet_name)
                        except Exception as e:
                            logger.warning(f"å¯¼å‡ºå·²åˆ é™¤é¡¹ç›®å¤±è´¥: {e}")

                excel_content = output_buffer.getvalue()
                if excel_content:
                    excel_base64 = base64.b64encode(excel_content).decode("utf-8")

            # --- å‘é€ç»“æœ ---
            if not excel_base64 and not zip_base64:
                yield event.plain_result("âŒ æœªèƒ½å¯¼å‡ºä»»ä½•æ•°æ®ã€‚")
                return

            # å‘é€ Excel
            if excel_base64:
                excel_options = [opt for opt in requested_options if opt != "ç¾¤ç›¸å†Œ"]
                if len(excel_options) == 1:
                    type_str = excel_options[0]
                else:
                    type_str = "ç¾¤æ•°æ®"
                
                excel_name = f"ç¾¤{group_id}_{type_str}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                # è·å–å½“å‰æŒ‡ä»¤å‘å‡ºçš„ç¯å¢ƒç¾¤å·ï¼Œç”¨äºæ–‡ä»¶ä¸Šä¼ 
                current_context_group_id = event.get_group_id()
                if event.message_obj.type == MessageType.GROUP_MESSAGE and current_context_group_id:
                    await client.upload_group_file(group_id=int(current_context_group_id), file=f"base64://{excel_base64}", name=excel_name)
                else:
                    await client.upload_private_file(user_id=int(event.get_sender_id()), file=f"base64://{excel_base64}", name=excel_name)
            
            # å‘é€ç›¸å†Œ ZIP
            if zip_base64:
                zip_name = f"ç¾¤{group_id}_ç¾¤ç›¸å†Œ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
                current_context_group_id = event.get_group_id()
                if event.message_obj.type == MessageType.GROUP_MESSAGE and current_context_group_id:
                    await client.upload_group_file(group_id=int(current_context_group_id), file=f"base64://{zip_base64}", name=zip_name)
                else:
                    await client.upload_private_file(user_id=int(event.get_sender_id()), file=f"base64://{zip_base64}", name=zip_name)

            yield event.plain_result(f"âœ… ç¾¤ {group_id} æ•°æ®å¯¼å‡ºæˆåŠŸï¼Œæ–‡ä»¶å·²ä¸Šä¼ ã€‚")

        except Exception as e:
            logger.error(f"ç¾¤å¯¼å‡ºå‡ºé”™: {e}")
            yield event.plain_result(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

    @filter.command("ç¾¤æ¢å¤")
    async def group_restore(self, event: AstrMessageEvent, group_id_arg: str = ""):
        """ç¾¤æ¢å¤ [ç¾¤å·]ï¼šå°†æŒ‡å®šç¾¤æˆ–å½“å‰ç¾¤çš„å¤‡ä»½æ•°æ®æ¢å¤åˆ°å½“å‰ç¾¤"""
        # æƒé™æ£€æŸ¥
        sender_id = int(event.get_sender_id())
        if self.admin_users and sender_id not in self.admin_users:
            yield event.plain_result("âŒ æ‚¨æ²¡æœ‰æƒé™æ‰§è¡Œæ­¤æŒ‡ä»¤ã€‚")
            return

        current_group_id = event.get_group_id()
        if not current_group_id:
            yield event.plain_result("âŒ è¯·åœ¨ç¾¤èŠä¸­ä½¿ç”¨æ­¤æŒ‡ä»¤ã€‚")
            return
        current_group_id = int(current_group_id)

        # ç¡®å®šå¤‡ä»½æ¥æºç¾¤å·
        source_group_id = int(group_id_arg) if group_id_arg and group_id_arg.isdigit() else current_group_id

        try:
            client = event.bot
            yield event.plain_result(f"æ­£åœ¨ä»ç¾¤ {source_group_id} çš„å¤‡ä»½æ¢å¤æ•°æ®åˆ°å½“å‰ç¾¤...")

            # 1. åŠ è½½å¤‡ä»½æ•°æ®
            latest_data = self._get_latest_backup_data(source_group_id)
            if not latest_data:
                yield event.plain_result(f"âŒ æœªæ‰¾åˆ°ç¾¤ {source_group_id} çš„å¤‡ä»½æ•°æ®ã€‚")
                return

            restore_options = self.restore_options
            group_info = latest_data.get("group_detail", {})
            
            # 2. æ¢å¤ç¾¤åç§°
            if "ç¾¤åç§°" in restore_options:
                new_name = group_info.get("groupName")
                if new_name:
                    logger.info(f"æ­£åœ¨æ¢å¤ç¾¤åç§°: {new_name}")
                    await client.set_group_name(group_id=current_group_id, group_name=new_name)
                    logger.info("ç¾¤åç§°æ¢å¤å®Œæˆ")
                else:
                    logger.warning("å¤‡ä»½æ•°æ®ä¸­æœªæ‰¾åˆ°ç¾¤åç§°ï¼Œè·³è¿‡æ¢å¤")

            # 3. æ¢å¤ç¾¤å¤´åƒ
            if "ç¾¤å¤´åƒ" in restore_options:
                # å°è¯•ä»å¤‡ä»½ç›®å½•æŸ¥æ‰¾å¤´åƒæ–‡ä»¶ï¼Œä¼˜å…ˆæ‰¾ group_avatar.png
                avatar_path = Path(self.plugin_data_dir) / str(source_group_id) / "group_avatar.png"
                if not avatar_path.exists():
                    avatar_path = Path(self.plugin_data_dir) / str(source_group_id) / "avatar.png"
                if not avatar_path.exists():
                    avatar_path = Path(self.plugin_data_dir) / str(source_group_id) / "avatar.jpg"
                
                if avatar_path.exists():
                    logger.info(f"æ­£åœ¨æ¢å¤ç¾¤å¤´åƒ: {avatar_path}")
                    await client.set_group_portrait(group_id=current_group_id, file=f"file://{avatar_path.absolute()}")
                    logger.info("ç¾¤å¤´åƒæ¢å¤å®Œæˆ")
                else:
                    logger.warning(f"æœªæ‰¾åˆ°å¤‡ä»½çš„ç¾¤å¤´åƒæ–‡ä»¶ (å°è¯•è¿‡ group_avatar.png, avatar.png, avatar.jpg): {avatar_path}")

            # 4. æ¢å¤ç¾¤æˆå‘˜è®¾ç½® (æ˜µç§°ã€å¤´è¡”ã€ç®¡ç†å‘˜)
            if any(opt in restore_options for opt in ["ç¾¤æ˜µç§°", "ç¾¤å¤´è¡”", "ç¾¤ç®¡ç†"]):
                backup_members = latest_data.get("members", [])
                if backup_members:
                    # è·å–å½“å‰ç¾¤æˆå‘˜åˆ—è¡¨
                    current_members_raw = await client.get_group_member_list(group_id=current_group_id)
                    current_member_ids = {m.get("user_id") for m in current_members_raw} if current_members_raw else set()
                    
                    restore_count = 0
                    for bm in backup_members:
                        user_id = bm.get("user_id")
                        if user_id not in current_member_ids:
                            continue
                        
                        # æ¢å¤ç¾¤æ˜µç§° (åç‰‡)
                        if "ç¾¤æ˜µç§°" in restore_options and "card" in bm:
                            await client.set_group_card(group_id=current_group_id, user_id=user_id, card=bm["card"])
                        
                        # æ¢å¤ç¾¤å¤´è¡”
                        if "ç¾¤å¤´è¡”" in restore_options and "special_title" in bm:
                            await client.set_group_special_title(group_id=current_group_id, user_id=user_id, special_title=bm["special_title"])
                        
                        # æ¢å¤ç¾¤ç®¡ç†
                        if "ç¾¤ç®¡ç†" in restore_options and "role" in bm:
                            is_admin = bm["role"] == "admin"
                            if bm["role"] != "owner":
                                await client.set_group_admin(group_id=current_group_id, user_id=user_id, enable=is_admin)
                        
                        restore_count += 1
                        if restore_count % 10 == 0:
                            logger.info(f"å·²æ¢å¤ {restore_count} åæˆå‘˜çš„è®¾ç½®...")

                    logger.info(f"ç¾¤æˆå‘˜è®¾ç½®æ¢å¤å®Œæˆ (å…± {restore_count} äºº)")

            # 5. æ¢å¤ç¾¤ç›¸å†Œ
            if "ç¾¤ç›¸å†Œ" in restore_options:
                backup_albums = latest_data.get("albums", [])
                backup_album_media = latest_data.get("album_media", {})
                
                if backup_albums:
                    # è·å–å½“å‰ç¾¤ç›¸å†Œåˆ—è¡¨ï¼Œç”¨äºæ¯”å¯¹åŒåç›¸å†Œ
                    try:
                        current_albums = await client.get_qun_album_list(group_id=str(current_group_id))
                    except:
                        current_albums = []
                    
                    album_name_to_id = {a.get("name"): a.get("album_id") for a in current_albums}
                    
                    for album in backup_albums:
                        album_name = album.get("name")
                        album_id = album.get("album_id")
                        
                        if album_name not in album_name_to_id:
                            logger.warning(f"å½“å‰ç¾¤ä¸å­˜åœ¨ç›¸å†Œ '{album_name}'ï¼Œè¯·å…ˆæ‰‹åŠ¨åˆ›å»ºåŒåç›¸å†Œã€‚è·³è¿‡æ­¤ç›¸å†Œæ¢å¤ã€‚")
                            continue
                        
                        target_album_id = album_name_to_id[album_name]
                        media_list = backup_album_media.get(album_id, [])
                        
                        if not media_list:
                            continue
                        
                        # è·å–ç›®æ ‡ç›¸å†Œå·²æœ‰çš„åª’ä½“åˆ—è¡¨ï¼Œé¿å…é‡å¤ä¸Šä¼ 
                        try:
                            target_media_raw = await client.get_group_album_media_list(group_id=str(current_group_id), album_id=target_album_id)
                            
                            existing_media_ids = set()
                            
                            # å¦‚æœè¿”å›çš„æ˜¯å­—å…¸ä¸”åŒ…å«åˆ—è¡¨å­—æ®µï¼Œå°è¯•æå–
                            media_items = []
                            if isinstance(target_media_raw, list):
                                media_items = target_media_raw
                            elif isinstance(target_media_raw, dict):
                                media_items = target_media_raw.get("media_list", target_media_raw.get("list", []))
                            
                            for m in media_items:
                                # å°è¯•æå–å„ç§å¯èƒ½çš„ ID
                                mid = m.get("media_id") or m.get("id")
                                if not mid and m.get("image"): mid = m.get("image", {}).get("lloc")
                                if not mid and m.get("video"): mid = m.get("video", {}).get("id")
                                if mid: existing_media_ids.add(str(mid))
                        except Exception as e:
                            logger.error(f"è·å–ç›¸å†Œåª’ä½“åˆ—è¡¨å¤±è´¥: {e}")
                            existing_media_ids = set()

                        # æ¢å¤å›¾ç‰‡
                        album_path = Path(self.plugin_data_dir) / str(source_group_id) / "albums" / album_name
                        if not album_path.exists():
                            continue
                        
                        upload_count = 0
                        for m in media_list:
                            # ä»…æ”¯æŒå›¾ç‰‡æ¢å¤ï¼Œè·³è¿‡è§†é¢‘ (media_type == 1)
                            if m.get("media_type") != 0:
                                continue
                                
                            m_id = str(m.get("media_id"))
                            if m_id in existing_media_ids:
                                # logger.debug(f"è·³è¿‡å·²å­˜åœ¨åª’ä½“: {m_id}")
                                continue
                            
                            file_ext = ".jpg" 
                            local_file = album_path / f"{m_id}{file_ext}"
                            
                            if local_file.exists():
                                try:
                                    # è°ƒç”¨ä¸Šä¼  API
                                    await client.upload_image_to_qun_album(
                                        group_id=str(current_group_id),
                                        album_id=target_album_id,
                                        album_name=album_name,
                                        file=f"file://{local_file.absolute()}"
                                    )
                                    upload_count += 1
                                    if upload_count % 5 == 0:
                                        logger.info(f"ç›¸å†Œ '{album_name}' å·²ä¸Šä¼  {upload_count} ä¸ªæ–‡ä»¶...")
                                except Exception as e:
                                    logger.error(f"ä¸Šä¼ æ–‡ä»¶ {local_file} åˆ°ç›¸å†Œå¤±è´¥: {e}")

                        logger.info(f"ç›¸å†Œ '{album_name}' æ¢å¤å®Œæˆ (ä¸Šä¼  {upload_count} ä¸ªæ–°æ–‡ä»¶)")

            yield event.plain_result(f"âœ… ç¾¤æ•°æ®æ¢å¤ä»»åŠ¡å·²æ‰§è¡Œå®Œæ¯•ã€‚")

        except Exception as e:
            logger.error(f"ç¾¤æ¢å¤å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield event.plain_result(f"âŒ æ¢å¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
